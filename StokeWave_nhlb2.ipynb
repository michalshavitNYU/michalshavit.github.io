{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalshavitNYU/michalshavitnyu.github.io/blob/master/StokeWave_nhlb2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwq3f-JZbEXl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab1c041-b3f9-4a9a-e3d6-174ae117c678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhJjfQ3gcHe2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90b5f90f-5fca-483f-e539-2da64a9d92f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optax==0.1.7\n",
            "  Downloading optax-0.1.7-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax==0.1.7) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax==0.1.7) (0.1.87)\n",
            "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.10/dist-packages (from optax==0.1.7) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax==0.1.7) (0.4.33)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from optax==0.1.7) (1.26.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax==0.1.7) (4.12.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax==0.1.7) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax==0.1.7) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax==0.1.7) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.1.55->optax==0.1.7) (1.13.1)\n",
            "Downloading optax-0.1.7-py3-none-any.whl (154 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/154.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.1/154.1 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: optax\n",
            "  Attempting uninstall: optax\n",
            "    Found existing installation: optax 0.2.4\n",
            "    Uninstalling optax-0.2.4:\n",
            "      Successfully uninstalled optax-0.2.4\n",
            "Successfully installed optax-0.1.7\n",
            "Collecting flax==0.7.5\n",
            "  Downloading flax-0.7.5-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from flax==0.7.5) (1.26.4)\n",
            "Requirement already satisfied: jax>=0.4.19 in /usr/local/lib/python3.10/dist-packages (from flax==0.7.5) (0.4.33)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax==0.7.5) (1.1.0)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax==0.7.5) (0.1.7)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.10/dist-packages (from flax==0.7.5) (0.6.4)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax==0.7.5) (0.1.68)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.7.5) (13.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.10/dist-packages (from flax==0.7.5) (4.12.2)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax==0.7.5) (6.0.2)\n",
            "Requirement already satisfied: jaxlib<=0.4.33,>=0.4.33 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax==0.7.5) (0.4.33)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax==0.7.5) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax==0.7.5) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.19->flax==0.7.5) (1.13.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.7.5) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax==0.7.5) (2.18.0)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from optax->flax==0.7.5) (1.4.0)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax==0.7.5) (0.1.87)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax==0.7.5) (1.10.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax==0.7.5) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax==0.7.5) (4.25.5)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint->flax==0.7.5) (4.11.0)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax==0.7.5) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax==0.7.5) (0.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.7.5) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.7.5) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax==0.7.5) (3.21.0)\n",
            "Downloading flax-0.7.5-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.4/244.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: flax\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.8.5\n",
            "    Uninstalling flax-0.8.5:\n",
            "      Successfully uninstalled flax-0.8.5\n",
            "Successfully installed flax-0.7.5\n",
            "Collecting chex==0.1.85\n",
            "  Downloading chex-0.1.85-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: absl-py>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex==0.1.85) (1.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex==0.1.85) (4.12.2)\n",
            "Requirement already satisfied: jax>=0.4.16 in /usr/local/lib/python3.10/dist-packages (from chex==0.1.85) (0.4.33)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from chex==0.1.85) (0.4.33)\n",
            "Requirement already satisfied: numpy>=1.24.1 in /usr/local/lib/python3.10/dist-packages (from chex==0.1.85) (1.26.4)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex==0.1.85) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.16->chex==0.1.85) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.16->chex==0.1.85) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.16->chex==0.1.85) (1.13.1)\n",
            "Downloading chex-0.1.85-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: chex\n",
            "  Attempting uninstall: chex\n",
            "    Found existing installation: chex 0.1.87\n",
            "    Uninstalling chex-0.1.87:\n",
            "      Successfully uninstalled chex-0.1.87\n",
            "Successfully installed chex-0.1.85\n",
            "Collecting orbax-checkpoint==0.4.7\n",
            "  Downloading orbax_checkpoint-0.4.7-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (1.4.0)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (1.10.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (4.12.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (1.1.0)\n",
            "Requirement already satisfied: jax>=0.4.9 in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (0.4.33)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (1.26.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (6.0.2)\n",
            "Requirement already satisfied: tensorstore>=0.1.51 in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (0.1.68)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (1.6.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from orbax-checkpoint==0.4.7) (4.25.5)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.9->orbax-checkpoint==0.4.7) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.9->orbax-checkpoint==0.4.7) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.9->orbax-checkpoint==0.4.7) (1.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.4.7) (2024.10.0)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.4.7) (6.4.5)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[epath,epy]->orbax-checkpoint==0.4.7) (3.21.0)\n",
            "Downloading orbax_checkpoint-0.4.7-py3-none-any.whl (129 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.2/129.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: orbax-checkpoint\n",
            "  Attempting uninstall: orbax-checkpoint\n",
            "    Found existing installation: orbax-checkpoint 0.6.4\n",
            "    Uninstalling orbax-checkpoint-0.6.4:\n",
            "      Successfully uninstalled orbax-checkpoint-0.6.4\n",
            "Successfully installed orbax-checkpoint-0.4.7\n",
            "Collecting scipy==1.11.4\n",
            "  Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy==1.11.4) (1.26.4)\n",
            "Downloading scipy-1.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.4/36.4 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "Successfully installed scipy-1.11.4\n",
            "Collecting jax==0.4.26\n",
            "  Downloading jax-0.4.26-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting jax-cuda12-pjrt==0.4.26\n",
            "  Downloading jax_cuda12_pjrt-0.4.26-py3-none-manylinux2014_x86_64.whl.metadata (349 bytes)\n",
            "Collecting jax-cuda12-plugin==0.4.26\n",
            "  Downloading jax_cuda12_plugin-0.4.26-cp310-cp310-manylinux2014_x86_64.whl.metadata (560 bytes)\n",
            "Collecting jaxlib==0.4.26\n",
            "  Downloading jaxlib-0.4.26-cp310-cp310-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.26) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.26) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax==0.4.26) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax==0.4.26) (1.11.4)\n",
            "Downloading jax-0.4.26-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_pjrt-0.4.26-py3-none-manylinux2014_x86_64.whl (85.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jax_cuda12_plugin-0.4.26-cp310-cp310-manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.26-cp310-cp310-manylinux2014_x86_64.whl (78.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jax-cuda12-pjrt, jax-cuda12-plugin, jaxlib, jax\n",
            "  Attempting uninstall: jax-cuda12-pjrt\n",
            "    Found existing installation: jax-cuda12-pjrt 0.4.33\n",
            "    Uninstalling jax-cuda12-pjrt-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-pjrt-0.4.33\n",
            "  Attempting uninstall: jax-cuda12-plugin\n",
            "    Found existing installation: jax-cuda12-plugin 0.4.33\n",
            "    Uninstalling jax-cuda12-plugin-0.4.33:\n",
            "      Successfully uninstalled jax-cuda12-plugin-0.4.33\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.4.33\n",
            "    Uninstalling jaxlib-0.4.33:\n",
            "      Successfully uninstalled jaxlib-0.4.33\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.4.33\n",
            "    Uninstalling jax-0.4.33:\n",
            "      Successfully uninstalled jax-0.4.33\n",
            "Successfully installed jax-0.4.26 jax-cuda12-pjrt-0.4.26 jax-cuda12-plugin-0.4.26 jaxlib-0.4.26\n",
            "Collecting kfac_jax==0.0.6\n",
            "  Downloading kfac_jax-0.0.6-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py>=0.12.0 in /usr/local/lib/python3.10/dist-packages (from kfac_jax==0.0.6) (1.4.0)\n",
            "Requirement already satisfied: immutabledict>=2.2.1 in /usr/local/lib/python3.10/dist-packages (from kfac_jax==0.0.6) (4.2.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.10/dist-packages (from kfac_jax==0.0.6) (1.26.4)\n",
            "Collecting distrax>=0.1.3 (from kfac_jax==0.0.6)\n",
            "  Downloading distrax-0.1.5-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jax>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from kfac_jax==0.0.6) (0.4.26)\n",
            "Requirement already satisfied: jaxlib>=0.4.7 in /usr/local/lib/python3.10/dist-packages (from kfac_jax==0.0.6) (0.4.26)\n",
            "Requirement already satisfied: dm-tree>=0.1.7 in /usr/local/lib/python3.10/dist-packages (from kfac_jax==0.0.6) (0.1.8)\n",
            "Requirement already satisfied: optax>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from kfac_jax==0.0.6) (0.1.7)\n",
            "Requirement already satisfied: chex>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from distrax>=0.1.3->kfac_jax==0.0.6) (0.1.85)\n",
            "Requirement already satisfied: tensorflow-probability>=0.15.0 in /usr/local/lib/python3.10/dist-packages (from distrax>=0.1.3->kfac_jax==0.0.6) (0.24.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.7->kfac_jax==0.0.6) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.7->kfac_jax==0.0.6) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax>=0.4.7->kfac_jax==0.0.6) (1.11.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.8->distrax>=0.1.3->kfac_jax==0.0.6) (4.12.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.8->distrax>=0.1.3->kfac_jax==0.0.6) (0.12.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.3->kfac_jax==0.0.6) (1.16.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.3->kfac_jax==0.0.6) (4.4.2)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.3->kfac_jax==0.0.6) (3.1.0)\n",
            "Requirement already satisfied: gast>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability>=0.15.0->distrax>=0.1.3->kfac_jax==0.0.6) (0.6.0)\n",
            "Downloading kfac_jax-0.0.6-py3-none-any.whl (146 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.2/146.2 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distrax-0.1.5-py3-none-any.whl (319 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: distrax, kfac_jax\n",
            "Successfully installed distrax-0.1.5 kfac_jax-0.0.6\n"
          ]
        }
      ],
      "source": [
        "!pip install optax==0.1.7\n",
        "\n",
        "!pip install flax==0.7.5\n",
        "\n",
        "!pip install chex==0.1.85\n",
        "\n",
        "!pip install orbax-checkpoint==0.4.7\n",
        "\n",
        "!pip install scipy==1.11.4\n",
        "\n",
        "!pip install jax==0.4.26 jax-cuda12-pjrt==0.4.26 jax-cuda12-plugin==0.4.26 jaxlib==0.4.26\n",
        "\n",
        "!pip install kfac_jax==0.0.6\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BShsl78Gb6Zl"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import optax\n",
        "import abc\n",
        "from jax import random, vjp, vmap\n",
        "from jax.tree_util import tree_map\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from scipy.io import savemat\n",
        "from scipy.interpolate import interp1d\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "\n",
        "from kfac_jax import curvature_estimator\n",
        "from kfac_jax import curvature_blocks\n",
        "from kfac_jax import layers_and_loss_tags\n",
        "from kfac_jax import loss_functions as kfac_loss_functions\n",
        "from kfac_jax import optimizer as kfac_optim\n",
        "\n",
        "# change JAX to double precision\n",
        "jax.config.update('jax_enable_x64', True)\n",
        "\n",
        "# define the root path\n",
        "rootdir = Path('/content/drive').joinpath('MyDrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRhA3YtVDpEf"
      },
      "source": [
        "# Customized function for collocation point arrangement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28CM-ZkmAm2I"
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "import jax.scipy as jsp\n",
        "\n",
        "def gaussian1D_smooth(f, sig, wid):\n",
        "    '''\n",
        "    :param f: equally spaced 1D position matrix  (N, 1)\n",
        "    :param sig: stan. devi of gaussian filter (1, ) or scalor\n",
        "    :param wid: wid of the filter matrix (1, ) or scalor integer\n",
        "    '''\n",
        "    wid = jnp.int32(wid)\n",
        "    xg = jnp.linspace(-sig, sig, wid)\n",
        "    window = jsp.stats.norm.pdf(xg)\n",
        "    win_n = window / jnp.sum(window)\n",
        "    f_smooth = scipy.signal.convolve(f[:, 0], win_n, mode='same')[:, None]\n",
        "    return f_smooth\n",
        "\n",
        "\n",
        "# sample the data based on a given probability distribution\n",
        "def colloc1D_set(key, x, f, Ns):\n",
        "    '''\n",
        "    :param x: 1-D position array (N, 1)\n",
        "    :param f: 1-D distribution array (N, 1)\n",
        "    :param Ns: number of points to sample\n",
        "    '''\n",
        "    # remove last element in each direction\n",
        "    xc = x[0:-1, :]\n",
        "    fc = f[0:-1, :]\n",
        "    dx = xc[1] - xc[0]\n",
        "    seq = jnp.arange(fc.shape[0] + 1)\n",
        "    # generate key for random variables\n",
        "    keys = jax.random.split(key, num=2)\n",
        "\n",
        "    # obtain the cumulative sum of the z value\n",
        "    b = jnp.hstack([0., jnp.cumsum(fc)])\n",
        "    # obtain the random variable\n",
        "    c = jax.random.uniform(keys[0], [Ns]) * b[-1]\n",
        "    # generate the index position of each collocation point following the distribution\n",
        "    # (using the interpolate the index of grid where each random variable stands)\n",
        "    posi_intp = jnp.interp(c, b, seq)\n",
        "    # round the result to guarantee that the index position is integer\n",
        "    posi = jnp.int32(jnp.floor(posi_intp))\n",
        "    # obtain the real position of each collocation point\n",
        "    px = xc[posi, :]\n",
        "    # generate a random fraction for each collocation point\n",
        "    posi_add = jax.random.uniform(keys[1], [c.shape[0], 1])\n",
        "    # add the random fraction to the position of each collocation points\n",
        "    x_col = px + posi_add * dx\n",
        "\n",
        "    return x_col\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MMaY9XmDtS1"
      },
      "source": [
        "# Weight initialization and network architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pKuH0Az4l2ut"
      },
      "outputs": [],
      "source": [
        "# initialize the neural network weights and biases\n",
        "def init_MLP(parent_key, layer_widths):\n",
        "    params = []\n",
        "    keys = random.split(parent_key, num=len(layer_widths) - 1)\n",
        "    # create the weights and biases for the network\n",
        "    for in_dim, out_dim, key in zip(layer_widths[:-1], layer_widths[1:], keys):\n",
        "        weight_key, bias_key = random.split(key)\n",
        "        xavier_stddev = jnp.sqrt(2 / (in_dim + out_dim))\n",
        "        params.append(\n",
        "            [random.truncated_normal(weight_key, -2, 2, shape=(in_dim, out_dim)) * xavier_stddev,\n",
        "             random.truncated_normal(bias_key, -2, 2, shape=(out_dim,)) * xavier_stddev]\n",
        "        )\n",
        "    return params\n",
        "\n",
        "\n",
        "# generate weights and biases for all variables of CLM problem\n",
        "def stwv_init_MLP(parent_key, n_hl, n_unit):\n",
        "    '''\n",
        "    :param n_hl: number of hidden layers [int]\n",
        "    :param n_unit: number of units in each layer [int]\n",
        "    '''\n",
        "    layers = [1] + n_hl * [n_unit] + [1]\n",
        "    # generate the random key for each network\n",
        "    pkey, *keys = random.split(parent_key, 1 + 1)\n",
        "    # generate weights and biases for\n",
        "    wb = tree_map(lambda x: init_MLP(x, layers), keys)\n",
        "    # generate the free para for lambda\n",
        "    lamb_mod = jnp.array([0.], dtype='float64')\n",
        "    # group all the parameter\n",
        "    params = wb + [lamb_mod]\n",
        "    return params\n",
        "\n",
        "\n",
        "def linfun(x):\n",
        "    return x\n",
        "\n",
        "\n",
        "# define sech function\n",
        "def sech(z):\n",
        "    return 1 / jnp.cosh(z)\n",
        "\n",
        "\n",
        "def network_core(params, H):\n",
        "    # separate the first, hidden and last layers\n",
        "    first, *hidden, last = params\n",
        "    # calculate the first layers output with right scale\n",
        "    H = jnp.tanh(jnp.dot(H, first[0]) + first[1])\n",
        "    # calculate the middle layers output\n",
        "    for layer in hidden:\n",
        "        H = jnp.tanh(jnp.dot(H, layer[0]) + layer[1])\n",
        "    # no activation function for last layer\n",
        "    var = jnp.dot(H, last[0]) + last[1]\n",
        "    return var\n",
        "\n",
        "\n",
        "# define the basic formation of neural network\n",
        "def neural_net_even(params, x, limit):\n",
        "    '''\n",
        "    :param params: weights and biases\n",
        "    :param limit: characteristic scale for normalization [matrix with shape [2, m]]\n",
        "    :return: neural network output [matrix with shape [N, n]]; n is number of outputs)\n",
        "    '''\n",
        "    pd = limit[0]  # half period in x and length in y\n",
        "    # normalize the input\n",
        "    th = x / pd\n",
        "    # double the variable in r direction\n",
        "    H = jnp.cos(jnp.pi * th) # even function with respect to x = 0\n",
        "    # calculate the output for all networks\n",
        "    var0 = network_core(params, H)\n",
        "    return var0\n",
        "\n",
        "\n",
        "def stwv_pred_create(limit):\n",
        "    '''\n",
        "    :param limit: domain size of the input\n",
        "    :return: function of the solution (a callable)\n",
        "    '''\n",
        "    pd, scl = limit  # half period in x and length in y\n",
        "    # high-level architecture for IPM solution\n",
        "    def f(params, x):\n",
        "        # calculate the raw network output\n",
        "        y = scl * neural_net_even(params[0], x, limit)\n",
        "        return y\n",
        "    return f\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuDT4IhlW-TQ"
      },
      "source": [
        "# numiercal Periodic Hilbert Transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huxyGaiMXB00"
      },
      "outputs": [],
      "source": [
        "\"\"\"High-precision numerical calculation of Hilbert *transform*\"\"\"\n",
        "\n",
        "\n",
        "def hilb_bd_detect(x, xn, n):\n",
        "    # detect the position of data in xn that is next to x\n",
        "    vn0 = xn - x\n",
        "    tn = jnp.tan(vn0 / 2)\n",
        "    # convert the vn to the new range from -pi to pi\n",
        "    vn = 2 * jnp.arctan(tn)   # very important step\n",
        "    # compute the p variable based on the new vn\n",
        "    pn = jnp.sin(vn / 2)\n",
        "    # find the least positive and most negative points\n",
        "    tn0 = tn - (tn > -1e-16) * jnp.inf\n",
        "    tn1 = tn + (tn < 1e-16) * jnp.inf\n",
        "    idxb0 = jnp.argmin(-tn0)\n",
        "    idxb1 = jnp.argmin(tn1)\n",
        "    # distance of those point withe respect zero\n",
        "    hp0 = -pn[idxb0]\n",
        "    hp1 = pn[idxb1]\n",
        "    hpm = (hp0 + hp1)/2\n",
        "    # prepare the nodes for inner domain integration with respect to vn\n",
        "    p_front = -jnp.arange(n, 0, -2) * hpm / n\n",
        "    p_rear = jnp.arange(1, n + 1, 2) * hpm / n\n",
        "    p_inner = jnp.hstack([p_front, p_rear])\n",
        "    # convert the node back to original coordinate\n",
        "    x_in = jnp.arcsin(p_inner) * 2 + x\n",
        "\n",
        "    # detect the boundary for the closer region\n",
        "    tnc0 = tn - (tn >= -15 * hpm) * jnp.inf\n",
        "    tnc1 = tn + (tn <= 15 * hpm) * jnp.inf\n",
        "    idxc0 = jnp.argmin(-tnc0)\n",
        "    idxc1 = jnp.argmin(tnc1)\n",
        "    # compute the tn value for the closest point\n",
        "    tn_cl = jnp.array([tn[idxc0], tn[idxc1]])\n",
        "    # compute the pn value for the closest point\n",
        "    pn0_cl = pn[idxc0]\n",
        "    pn1_cl = pn[idxc1]\n",
        "    # compute the qn value for the closest point to pn_bd[0]\n",
        "    qn0_cl = jnp.flip(jnp.log(-pn0_cl))\n",
        "    qn1_cl = jnp.log(pn1_cl)\n",
        "    # compute the qn value for the inner point\n",
        "    qn0_bd = jnp.flip(jnp.log(hpm))\n",
        "    qn1_bd = jnp.log(hpm)\n",
        "    # create the node point in qn for the closer region\n",
        "    qn0 = jnp.linspace(qn0_bd, qn0_cl, 8)\n",
        "    qn1 = jnp.linspace(qn1_bd, qn1_cl, 8)\n",
        "    return x_in, qn0, qn1, tn_cl\n",
        "\n",
        "\n",
        "def hilbtrans_close(x, q1n, q2n, func, params_c):\n",
        "    '''\n",
        "    hilbert transform of one single output using non-uniform sampling\n",
        "    based on Bilato's paper with O(h^2) accuracy\n",
        "    '''\n",
        "    # unroll the params\n",
        "    ni, wi = params_c\n",
        "    # compute the bound for each gaussian quadrature (GQ) integral\n",
        "    a1 = q1n[0:-1]\n",
        "    b1 = q1n[1:]\n",
        "    a2 = q2n[0:-1]\n",
        "    b2 = q2n[1:]\n",
        "    # normalize the gaussian quadrature range\n",
        "    q1i = (b1 - a1) / 2 * ni + (a1 + b1) / 2\n",
        "    q2i = (b2 - a2) / 2 * ni + (a2 + b2) / 2\n",
        "    # compute the corresponding x value for the GQ interpolation point\n",
        "    x1i = 2 * jnp.arcsin(-jnp.exp(q1i))\n",
        "    x2i = 2 * jnp.arcsin(jnp.exp(q2i))\n",
        "    x1i = x1i + x\n",
        "    x2i = x2i + x\n",
        "    # calculate the integrand of Gaussian quadrature\n",
        "    f1 = vmap(func, in_axes=0)(x1i[:, :, None])[:, :, 0]\n",
        "    f2 = -vmap(func, in_axes=0)(x2i[:, :, None])[:, :, 0]\n",
        "    # calculate using Gaussian Quadrature\n",
        "    close1 = (b1 - a1) / 2 * jnp.matmul(wi, f1)[0]\n",
        "    close2 = (b2 - a2) / 2 * jnp.matmul(wi, f2)[0]\n",
        "    close_all = jnp.hstack((close1, close2))\n",
        "    # sum all the sub-integral\n",
        "    closesum = jnp.sum(close_all)[None]\n",
        "    return closesum\n",
        "\n",
        "\n",
        "def hilbInt_outer(x, tn_cl, params_o):\n",
        "    '''\n",
        "    hilbert transform of one single output using non-uniform sampling\n",
        "    based on Bilato's paper with O(h^2) accuracy\n",
        "    '''\n",
        "    # unroll the params\n",
        "    xn, a, b, xi, fxi, ni, wi = params_o\n",
        "    # detect the position of data in xn that is next to x\n",
        "    vn = xn - x\n",
        "    tn = jnp.tan(vn / 2)\n",
        "    sg = (tn < tn_cl[0]-2e-16) | (tn >= tn_cl[1])\n",
        "    # calculate the integrand of Gaussian quadrature\n",
        "    fh = fxi / jnp.tan((x - xi) / 2) / 2\n",
        "    # calculate using Gaussian Quadrature\n",
        "    outer = (b-a) / 2 * jnp.matmul(wi, fh)[0] * sg[0:-1]\n",
        "    outsum = jnp.sum(outer)[None]\n",
        "    return outsum\n",
        "\n",
        "\n",
        "def hilb_perid_cubic(xn):\n",
        "    '''\n",
        "    hilbert transform of multiple outputs at any positions\n",
        "    within the range of sampling points\n",
        "    params func: function with respect to z (not x)\n",
        "    '''\n",
        "    # order of the method\n",
        "    n = 3\n",
        "    # flatten the sampling point (for output shape)\n",
        "    xn = xn.flatten()\n",
        "    # set the Gaussian Quadrature nodes  (order 9)\n",
        "    e1 = jnp.array(1 / 3 * jnp.sqrt(5 - 2 * jnp.sqrt(10 / 7)))\n",
        "    e2 = jnp.array(1 / 3 * jnp.sqrt(5 + 2 * jnp.sqrt(10 / 7)))\n",
        "    # set the Gaussian Quadrature weights (order 9)\n",
        "    c1 = jnp.array((322 - 13 * jnp.sqrt(70)) / 900)\n",
        "    c2 = jnp.array((322 + 13 * jnp.sqrt(70)) / 900)\n",
        "    c3 = jnp.array(128 / 225)\n",
        "    # group and match the weights and nodes\n",
        "    ni = jnp.vstack([-e2, -e1, 0, e1, e2])\n",
        "    wi = jnp.hstack([c1, c2, c3, c2, c1])[None, :]\n",
        "    # prepare the outer domain integration\n",
        "    a = xn[0:-1]\n",
        "    b = xn[1:]\n",
        "    # change of variables\n",
        "    xi = (b - a) / 2 * ni + (a + b) / 2\n",
        "\n",
        "    def hpfunc(x, func):\n",
        "        # obtain the position info for the inner and closer domain\n",
        "        x_in, q1n, q2n, tn_cl = vmap(hilb_bd_detect, in_axes=(0, None, None))(x, xn, n)\n",
        "        # prepare the nodes for inner domain integration\n",
        "        f_in = vmap(func, in_axes=0)(x_in[:, :, None])[:, :, 0]\n",
        "        # coefficient based on LG interpolation\n",
        "        c_in = jnp.array([1/4, 9/4, -9/4, -1/4])\n",
        "        # calculate the inner domain integration\n",
        "        Iinner = jnp.sum(c_in * f_in, axis=1)[:, None]\n",
        "\n",
        "        # group parameters required for close domain calculation\n",
        "        params_c = [ni, wi]\n",
        "        # calculation the close domain integration\n",
        "        Iclose = vmap(hilbtrans_close, in_axes=(0, 0, 0, None, None))(x, q1n, q2n, func, params_c)\n",
        "\n",
        "        # calculate the outer domain integration\n",
        "        fxi = vmap(func, in_axes=0)(xi[:, :, None])[:, :, 0]\n",
        "        params_o = [xn, a, b, xi, fxi, ni, wi]\n",
        "        # calculation the outer domain integration\n",
        "        Iouter = vmap(hilbInt_outer, in_axes=(0, 0, None))(x, tn_cl, params_o)\n",
        "        # Iouter = hilbInt_outer(x[0], tn_cl[0], params_o)\n",
        "        # sum up the components to the final expression\n",
        "        hfx = (Iinner + Iclose + Iouter) / jnp.pi\n",
        "        return hfx\n",
        "\n",
        "    return hpfunc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyETtFnOXPSJ"
      },
      "source": [
        "# Governing equation and Loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN6FnjnOXJu2"
      },
      "outputs": [],
      "source": [
        "\"\"\"Low-level functions developed for PINN training using JAX\"\"\"\n",
        "\n",
        "# define the mean squared error\n",
        "def ms_error(diff):\n",
        "    return jnp.mean(jnp.square(diff), axis=0)\n",
        "\n",
        "\n",
        "def ms_bias(diff, weight):\n",
        "    return jnp.sum(weight * jnp.square(diff))\n",
        "\n",
        "\n",
        "# generate matrix required for vjp for vector gradient\n",
        "def vgmat(z, n_out, idx=None):\n",
        "    '''\n",
        "    :param n_out: number of output variables\n",
        "    :param idx: indice (list) of the output variable to take the gradient\n",
        "    '''\n",
        "    if idx is None:\n",
        "        idx = range(n_out)\n",
        "    # obtain the number of index\n",
        "    n_idx = len(idx)\n",
        "    # obtain the number of input points\n",
        "    n_pt = z.shape[0]\n",
        "    # determine the shape of the gradient matrix\n",
        "    mat_shape = [n_idx, n_pt, n_out]\n",
        "    # create the zero matrix based on the shape\n",
        "    mat = jnp.zeros(mat_shape)\n",
        "    # choose the associated element in the matrix to 1\n",
        "    for l, ii in zip(range(n_idx), idx):\n",
        "        mat = mat.at[l, :, ii].set(1.)\n",
        "    return mat\n",
        "\n",
        "\n",
        "# batched multiplication between matrix (with different dimensions)\n",
        "def batch_mul(mat1, mat2):\n",
        "    # define the element-wise product function\n",
        "    prod = lambda x, y: jnp.multiply(x, y)\n",
        "    # define the batched matrix product (mat1 is one more dimension than mat2)\n",
        "    mat_prod = vmap(prod, (0, None))\n",
        "    return mat_prod(mat1, mat2)\n",
        "\n",
        "\n",
        "# vector gradient of the output with with input\n",
        "def vectgrad(func, z):\n",
        "    # obtain the output and the gradient function\n",
        "    sol, vjp_fn = vjp(func, z)\n",
        "    # determine the mat grad\n",
        "    mat = vgmat(z, sol.shape[1])\n",
        "    # calculate the gradient of each output with respect to each input\n",
        "    grad0 = vmap(vjp_fn, in_axes=0)(mat)[0]\n",
        "    # calculate the total partial derivative of output with input\n",
        "    n_pd = z.shape[1] * sol.shape[1]\n",
        "    # reshape the derivative of output with input\n",
        "    grad_s = grad0.transpose(1, 0, 2)\n",
        "    grad_all = grad_s.reshape(z.shape[0], n_pd)\n",
        "    return grad_all, sol\n",
        "\n",
        "\n",
        "# governing equation of Stokes equation\n",
        "def stwv_gov_eqn(f_y, x, lamb):\n",
        "    # calculate the output and its derivative with original coordinates\n",
        "    y_x, y = vectgrad(f_y, x)\n",
        "    f_yx = lambda x: vectgrad(f_y, x)[0]\n",
        "    f_ysq = lambda x: f_y(x) ** 2\n",
        "    f_ysqx = lambda x: vectgrad(f_ysq, x)[0]\n",
        "    # compute the hilbert transform\n",
        "    Hy_x = hptrans(x, f_yx)\n",
        "    Hysq_x = hptrans(x, f_ysqx)\n",
        "    # calculate the residue of the Stokes Wave equation\n",
        "    e = -lamb**2 * Hy_x + y + 0.5*Hysq_x + y*Hy_x\n",
        "    return e\n",
        "\n",
        "\n",
        "def stwv_deri_eqn(f_y, x, lamb):\n",
        "    # calculate the output and its derivative with original coordinates\n",
        "    y_x, y = vectgrad(f_y, x)\n",
        "    f_yx = lambda x: vectgrad(f_y, x)[0]\n",
        "    f_yxx = lambda x: vectgrad(f_yx, x)[0]\n",
        "    f_ysq = lambda x: f_y(x) ** 2\n",
        "    f_ysqx = lambda x: vectgrad(f_ysq, x)[0]\n",
        "    f_ysqxx = lambda x: vectgrad(f_ysqx, x)[0]\n",
        "\n",
        "    Hy_x = hptrans(x, f_yx)\n",
        "    Hy_xx = hptrans(x, f_yxx)\n",
        "    Hysq_xx = hptrans(x, f_ysqxx)\n",
        "    # calculate the stwv equation and its derivatives\n",
        "    e_d = -lamb**2 * Hy_xx + y_x + 0.5*Hysq_xx + y_x*Hy_x + y*Hy_xx\n",
        "    return e_d\n",
        "\n",
        "\n",
        "def loss_create(predf, cond, lamb0, lw, ew):\n",
        "    '''\n",
        "    a function factory to create the loss function based on given info\n",
        "    :param lamb0: initial guess of lambda\n",
        "    :return: a loss function (callable)\n",
        "    '''\n",
        "    # set the weight for each condition and equation\n",
        "    data_weight = jnp.array([1.])\n",
        "\n",
        "    # loss function used for the PINN training\n",
        "    def loss_fun(params, data):\n",
        "        # loading the pre-saved loss parameter\n",
        "        eta = data['eta']\n",
        "        lref = loss_fun.ref\n",
        "        eqn_weight = loss_fun.ew\n",
        "\n",
        "        # create the function for gradient calculation involves input Z only\n",
        "        func = lambda x: predf(params, x)\n",
        "        # update the lambda value\n",
        "        lamb = lamb0 + eta * jnp.tanh(params[-1])\n",
        "        # load the data of normalization condition\n",
        "        x_nm = cond['cond_nm'][0]\n",
        "        y_nm = cond['cond_nm'][1]\n",
        "\n",
        "        # load the position and weight of collocation points\n",
        "        x_c = data['col'][0]\n",
        "        w_c = data['col'][1]\n",
        "        # load the collocation points where we take gradient of equation\n",
        "        x_d = data['grad'][0]\n",
        "        w_d = data['grad'][1]\n",
        "\n",
        "        # calculate the gradient of phi at origin\n",
        "        y_nm_p = func(x_nm)[:, 0:1]\n",
        "\n",
        "        # calculate the residue of CCF equation and diff with HT\n",
        "        f_c = stwv_gov_eqn(func, x_c, lamb)\n",
        "        # calculate the residue of CCF derivative\n",
        "        f_d = stwv_deri_eqn(func, x_d, lamb)\n",
        "\n",
        "        # calculate the mean squared root error of normalization cond.\n",
        "        norm_err = ms_error(jax.nn.relu(y_nm - y_nm_p))  # y_nm_p need to larger than y_nm\n",
        "        # group all the data loss\n",
        "        data_err = jnp.hstack([norm_err])\n",
        "\n",
        "        # calculate the mean squared root error of equation\n",
        "        eqn_err_f = ms_bias(f_c, w_c)\n",
        "        eqn_err_df = ms_bias(f_d, w_d)\n",
        "        # group all the equation loss\n",
        "        eqn_err = jnp.hstack([eqn_err_f, eqn_err_df])\n",
        "\n",
        "        # register each loss function term\n",
        "        # ===========================================================\n",
        "        # ===========================================================\n",
        "        kfac_loss_functions.register_squared_error_loss(\n",
        "            jax.nn.relu(y_nm - y_nm_p), targets=jnp.zeros_like(y_nm_p),\n",
        "            weight=(data_weight[0] / y_nm_p.size / lref))\n",
        "\n",
        "        kfac_loss_functions.register_squared_error_loss(\n",
        "            f_c * jnp.sqrt(w_c), targets=jnp.zeros_like(f_c), weight=(lw * eqn_weight[0] / lref))\n",
        "\n",
        "        kfac_loss_functions.register_squared_error_loss(\n",
        "            f_d * jnp.sqrt(w_d), targets=jnp.zeros_like(f_d), weight=(lw * eqn_weight[1] / lref))\n",
        "\n",
        "        # ===========================================================\n",
        "        # ===========================================================\n",
        "\n",
        "        # calculate the overall data loss and equation loss\n",
        "        loss_data = jnp.sum(data_err * data_weight)\n",
        "        loss_eqn = jnp.sum(eqn_err * eqn_weight)\n",
        "\n",
        "        # calculate the total loss\n",
        "        loss = loss_data + lw * loss_eqn\n",
        "        loss_n = loss / lref\n",
        "        # group the loss of all conditions and equations\n",
        "        loss_info = jnp.hstack([jnp.array([loss, loss_data, loss_eqn]),\n",
        "                                data_err, eqn_err, lamb])\n",
        "        return loss_n, loss_info\n",
        "\n",
        "    # pre-setting the pre-saved loss parameter to loss_fun\n",
        "    loss_fun.ref = 1.\n",
        "    loss_fun.ew = ew\n",
        "    return loss_fun\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTf7ga8LXXd8"
      },
      "source": [
        "# Fast optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "03HN8AiQXVjM"
      },
      "outputs": [],
      "source": [
        "class KfacOptimizer(abc.ABC):\n",
        "  \"\"\"kfac_jax optimizer.\"\"\"\n",
        "\n",
        "  def __init__(self, loss_fn, **kwargs):\n",
        "\n",
        "    # loss_fn = functools.partial(\n",
        "    #     loss_fn,\n",
        "    #     curvature_block_grouping=kwargs.pop(\"curvature_block_grouping\"))\n",
        "\n",
        "    self._value_and_grad_func = jax.value_and_grad(loss_fn, has_aux=True)\n",
        "\n",
        "    self._norm_constraint = kwargs.pop(\"norm_constraint\")\n",
        "    self._curvature_block_type = kwargs.pop(\"curvature_block_type\")\n",
        "\n",
        "    learning_rate = kwargs.pop(\"learning_rate\")\n",
        "    momentum = kwargs.pop(\"momentum\")\n",
        "    damping = kwargs.pop(\"damping\")\n",
        "    self._initial_damping = kwargs.pop(\"initial_damping\")\n",
        "    self._min_damping = kwargs.pop(\"min_damping\")\n",
        "\n",
        "    if learning_rate is not None:\n",
        "      self._learning_rate_schedule = optax.constant_schedule(learning_rate)\n",
        "      self._use_adaptive_learning_rate = False\n",
        "    else:\n",
        "      self._learning_rate_schedule = None\n",
        "      self._norm_constraint = None\n",
        "      self._use_adaptive_learning_rate = True\n",
        "\n",
        "    if momentum is not None:\n",
        "      self._momentum_schedule = optax.constant_schedule(momentum)\n",
        "      self._use_adaptive_momentum = False\n",
        "    else:\n",
        "      self._momentum_schedule = None\n",
        "      self._norm_constraint = None\n",
        "      self._use_adaptive_momentum = True\n",
        "\n",
        "    if damping is not None:\n",
        "      self._damping_schedule = optax.constant_schedule(damping)\n",
        "      self._initial_damping = None\n",
        "      self._min_damping = None\n",
        "      self._use_adaptive_damping = False\n",
        "\n",
        "    else:\n",
        "      self._damping_schedule = None\n",
        "      self._use_adaptive_damping = True\n",
        "\n",
        "    self._extra_kwargs = kwargs\n",
        "\n",
        "  def get_optimizer(self):\n",
        "    \"\"\"Returns the kfac_jax optimizer as defined using optimizer config.\"\"\"\n",
        "\n",
        "    # should pass these to the optimizer directly instead of doing these\n",
        "    # module-level calls\n",
        "\n",
        "    if self._curvature_block_type == \"naive_full\":\n",
        "      curvature_estimator.set_default_tag_to_block_ctor(\n",
        "          \"generic_tag\", curvature_blocks.NaiveFull)\n",
        "\n",
        "    else:\n",
        "      raise ValueError(\n",
        "          f\"Unknown curvature block type: {self._curvature_block_type}.\")\n",
        "\n",
        "    self.optimizer = kfac_optim.Optimizer(\n",
        "        self._value_and_grad_func,\n",
        "        l2_reg=0.0,\n",
        "        value_func_has_aux=True,\n",
        "        use_adaptive_learning_rate=self._use_adaptive_learning_rate,\n",
        "        use_adaptive_momentum=self._use_adaptive_momentum,\n",
        "        use_adaptive_damping=self._use_adaptive_damping,\n",
        "        initial_damping=self._initial_damping,\n",
        "        min_damping=self._min_damping,\n",
        "        norm_constraint=self._norm_constraint,\n",
        "        learning_rate_schedule=self._learning_rate_schedule,\n",
        "        momentum_schedule=self._momentum_schedule,\n",
        "        damping_schedule=self._damping_schedule,\n",
        "        register_only_generic=True,\n",
        "        batch_size_extractor=lambda x: 1,\n",
        "        distributed_inverses=True,  # do not change this! (see comment above)\n",
        "        **self._extra_kwargs,\n",
        "        )\n",
        "\n",
        "    return self.optimizer\n",
        "\n",
        "\n",
        "kfac_config = dict(\n",
        "    # When learning_rate, momentum, and damping are each set to None\n",
        "    # this enables the respective adaptive methods for them.\n",
        "    # 1e-4, 0.9, and 1e-4 are sensible values, but the adaptive\n",
        "    # methods tend to gives better results. Only the damping\n",
        "    # adaptation method benefits from tuning\n",
        "    learning_rate=None,\n",
        "    momentum=None,\n",
        "    damping=None,\n",
        "    norm_constraint=1e-8,  # ignored when LR or momentum adaptation is used\n",
        "    initial_damping=1e-0,  # used by the adaptive damping method\n",
        "    min_damping=1e-15,  # used by the adaptive damping method\n",
        "    curvature_block_type=\"naive_full\",  # \"naive_full\" (very important setting)\n",
        "    damping_adaptation_decay=0.998,\n",
        "    curvature_ema=0.998,\n",
        "    inverse_update_period=1,\n",
        "    num_burnin_steps=2,  # TODO(jamesmartens): experiment with this\n",
        "    always_use_exact_qmodel_for_damping_adjustment=True,\n",
        "    include_norms_in_stats=True\n",
        "    )\n",
        "\n",
        "\n",
        "def kfac_optimizer(rng, lossf, config, params, dataf, F, epoch):\n",
        "    optim = KfacOptimizer(\n",
        "        loss_fn=lossf, **config).get_optimizer()\n",
        "\n",
        "    rng, key = jax.random.split(rng)\n",
        "    data_z = dataf(key, F)\n",
        "    weigh_z = weighf(wp=0.5)\n",
        "    data = dwcombine(data_z, weigh_z, eta0)\n",
        "\n",
        "    opt_state = optim.init(params, key, data)\n",
        "    loss_all = []\n",
        "\n",
        "    # start the training iteration\n",
        "    for step in range(epoch):\n",
        "        rng, *keys = jax.random.split(rng, 3)\n",
        "        params, opt_state, stats = optim.step(\n",
        "            params, opt_state, keys[0], batch=data, global_step_int=step)\n",
        "\n",
        "        loss_info = stats['aux']\n",
        "        loss_all.append(loss_info)\n",
        "        # get the equation loss\n",
        "        lamb_now = loss_info[-1]\n",
        "\n",
        "        if (step+1) % 50 == 0:\n",
        "            dmp = stats['damping']\n",
        "            print(f\"Step: {step+1} | Loss: {loss_info[0]:.4e} | Loss_d: {loss_info[1]:.4e} | \"\n",
        "                  f\"Loss_e: {loss_info[2]:.4e} | lamb: {lamb_now:.6f} | Dp: {dmp:.4e}\", file=sys.stderr)\n",
        "\n",
        "        if (step + 1) % 1000 == 0:\n",
        "            eta_now = data['eta']\n",
        "            F[0] = predictF(params, eta_now, dataf.X[0], mode=0)\n",
        "            F[1] = predictF(params, eta_now, dataf.X[1], mode=1)\n",
        "            data_z = dataf(keys[1], F)\n",
        "            data = dwcombine(data_z, weigh_z, eta_now)\n",
        "\n",
        "        # if (step + 1) == 5000:\n",
        "        #     weigh_z = weighf(wp=0.5)\n",
        "        #     data = dwcombine(data_z, weigh_z, eta1)\n",
        "\n",
        "    return params, loss_all\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGcUsNLBEtTe"
      },
      "source": [
        "# Dynamic data sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WKn4V4IEoJY"
      },
      "outputs": [],
      "source": [
        "\n",
        "def data_eqfunc1D(ds, ns, cen=None, lev=None, b=None):\n",
        "    def _norm_array(y):\n",
        "        lb = jnp.min(y)\n",
        "        ub = jnp.max(y)\n",
        "        return 2 * (y - lb) / (ub - lb) - 1\n",
        "\n",
        "    if cen == None:\n",
        "        cen = jnp.array([0.,])\n",
        "    else:\n",
        "        if jnp.array(cen).size == 1:\n",
        "            cen = jnp.array([cen])\n",
        "\n",
        "    if lev == None:\n",
        "        lev = jnp.array([0.,])\n",
        "    else:\n",
        "        if jnp.array(lev).size == 1:\n",
        "            lev = jnp.array([lev])\n",
        "\n",
        "    if b == None:\n",
        "        b = jnp.array([0.,])\n",
        "    else:\n",
        "        if jnp.array(b).size == 1:\n",
        "            b = jnp.array([b])\n",
        "\n",
        "    # different mode of distribution in the first direction\n",
        "    xrg = jnp.linspace(-1, 1, ns)\n",
        "\n",
        "    scl_x = jnp.max(jnp.abs(xrg-cen))\n",
        "    cum1 = jnp.cumsum(jnp.exp(-lev/scl_x * (xrg-cen) ** 2) + b)\n",
        "    cumn1 = _norm_array(cum1)\n",
        "    fnew1 = interp1d(cumn1, xrg)\n",
        "    z1mid = (ds[0] + ds[1]) / 2\n",
        "    z1ran = (ds[1] - ds[0]) / 2\n",
        "    z1 = fnew1(xrg) * z1ran + z1mid\n",
        "    return z1\n",
        "\n",
        "\n",
        "def data_func_create(N_fix, N_col, coeff=None):\n",
        "    if coeff is None:\n",
        "        coeff = [0., 0., 0.]\n",
        "\n",
        "    # =====================================================\n",
        "    # define the boundary at x-axis\n",
        "    ns_bc = 501\n",
        "    x_c = jnp.linspace(0, jnp.pi, ns_bc)[:, None]\n",
        "    F_c = 1 / (jnp.abs(x_c) + 0.1) ** coeff[0]\n",
        "\n",
        "    ns_bd = 501\n",
        "    x_d = jnp.linspace(0.5, jnp.pi, ns_bd)[:, None]\n",
        "    F_d = 1 / (jnp.abs(x_d) + 0.1) ** coeff[1]\n",
        "\n",
        "    ns_bm = 201\n",
        "    x_m = jnp.linspace(0, 0.5, ns_bm)[:, None]\n",
        "    F_m = 1 / (jnp.abs(x_m) + 0.1) ** coeff[2]\n",
        "\n",
        "    # generate the weights for fixed collocation points\n",
        "    wfx_c = jnp.ones([N_fix[0], 1]) / N_fix[0]\n",
        "    wfx_d = jnp.ones([N_fix[1], 1]) / N_fix[1]\n",
        "    wfx_m = jnp.ones([N_fix[2], 1]) / N_fix[2]\n",
        "\n",
        "    # generate the weights for boundary and collocation points\n",
        "    wmv_c = jnp.ones([N_col[0], 1]) / N_col[0]\n",
        "    wmv_d = jnp.ones([N_col[1], 1]) / N_col[1]\n",
        "\n",
        "    X = [x_c, x_d]\n",
        "    F0 = [F_c, F_d]\n",
        "\n",
        "    # define the function that can re-sampling for each calling\n",
        "    def dataf(key, F):\n",
        "        # generate the subkey for each group\n",
        "        new_keys = random.split(key, 10)\n",
        "\n",
        "        # fix distribution collocation point\n",
        "        xfx_c = colloc1D_set(new_keys[0], x_c, F_c, N_fix[0])\n",
        "        xfx_d = colloc1D_set(new_keys[1], x_d, F_d, N_fix[1])\n",
        "        xfx_m = colloc1D_set(new_keys[2], x_m, F_m, N_fix[2])\n",
        "\n",
        "        # prepare the collocation points based on a given distribution F\n",
        "        xmv_c = colloc1D_set(new_keys[5], x_c, F[0], N_col[0])\n",
        "        xmv_d = colloc1D_set(new_keys[6], x_d, F[1], N_col[1])\n",
        "\n",
        "        # group all the points evaluating residue\n",
        "        z_c = jnp.vstack([xfx_c, xfx_m, xmv_c])\n",
        "        # group all the points evaluating derivative of residue\n",
        "        z_d = jnp.vstack([xfx_d, xfx_m, xmv_d])\n",
        "        # group all the conditions and collocation points\n",
        "        data = [z_c, z_d]\n",
        "        return data\n",
        "\n",
        "    def weighf(wp):\n",
        "        w_c = jnp.vstack([wfx_c, wfx_m*wp, wmv_c]) * jnp.ones(1) / 1\n",
        "        w_d = jnp.vstack([wfx_d, wfx_m*wp, wmv_d]) * jnp.ones(1) / 1\n",
        "        return [w_c, w_d]\n",
        "\n",
        "    dataf.X = X\n",
        "    return dataf, weighf, F0\n",
        "\n",
        "\n",
        "# function to combine the weight and location for collocation point\n",
        "def dwcombine(data, weigh, eta=0.):\n",
        "    data_d = dict(col=[data[0], weigh[0]],\n",
        "                grad=[data[1], weigh[1]])\n",
        "    data_d['eta'] = eta\n",
        "    return data_d\n",
        "\n",
        "\n",
        "def predictF(params, eta, x, mode=0):\n",
        "    # get the inferred lamb\n",
        "    lamb = lamb0 + eta * jnp.tanh(params[-1])\n",
        "    # create the output function\n",
        "    func = lambda x: predf(params, x)\n",
        "\n",
        "    if mode == 0:\n",
        "        # calculate the equation residue\n",
        "        f0 = stwv_gov_eqn(func, x, lamb)\n",
        "    else:\n",
        "        # calculate the derivative of equation residue\n",
        "        f0 = stwv_deri_eqn(func, x, lamb)\n",
        "\n",
        "    # calculate the maximum of the square of residue for all equations\n",
        "    f_sq = jnp.max(jnp.vstack(f0)**2, axis=1)\n",
        "    # normalize the distribution function and add a basic level\n",
        "    f_nm = f_sq / jnp.mean(f_sq) + 0.5\n",
        "    # reshape the distribution function\n",
        "    F = jnp.reshape(f_nm, x.shape)  # F need to be column (N, 1)\n",
        "    # smooth the weight function by Gaussian filter\n",
        "    Fs = gaussian1D_smooth(F, 1., 5.)\n",
        "    return Fs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfXZEOv2E7dV"
      },
      "source": [
        "# Problem setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMLkUrKmo96N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 736
        },
        "outputId": "d2ef4b57-1f38-46bc-9f94-ac16f0c45fd4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.04889788514049958\n"
          ]
        }
      ],
      "source": [
        "# select the random seed\n",
        "seed = 1234\n",
        "key = random.PRNGKey(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# create the subkeys\n",
        "keys = random.split(key, 4)\n",
        "\n",
        "# select the size of neural network\n",
        "n_hl = 3\n",
        "n_unit = 30\n",
        "scl = 0.4\n",
        "pd = jnp.pi\n",
        "\n",
        "# number of sampling points\n",
        "N_smp = 4000\n",
        "N_fix = [251, 201, 51]  # [col, deri, mid]\n",
        "N_col = [121, 121]    # [col, deri]\n",
        "coeff = [0.8, 0., 0.]  # [col, deri, mid]\n",
        "\n",
        "# set the size of domain\n",
        "lmt = jnp.array([pd, scl])\n",
        "\n",
        "# initial guess of the lambda value\n",
        "lamb0 = 1. + scl**2/2\n",
        "\n",
        "# set the training iteration\n",
        "epoch2 = 40000\n",
        "lw = 0.1\n",
        "ew = jnp.array([1., 0.05])\n",
        "\n",
        "# training freedom\n",
        "eta0 = 0.\n",
        "eta1 = 0.\n",
        "\n",
        "# prepare the normalization condition\n",
        "x_nm = jnp.array([[0.]])\n",
        "cond_nm = jnp.array([[scl]])\n",
        "# group all the conditions and collocation points\n",
        "cond = dict(cond_nm=[x_nm, cond_nm])\n",
        "\n",
        "# prepare the sampling point for HT\n",
        "# x_smp0 = jnp.linspace(-1, 1, N_smp)\n",
        "# x_smp = jnp.sign(x_smp0) * jnp.abs(x_smp0) ** 1.4 * jnp.pi\n",
        "x_smp = data_eqfunc1D([-jnp.pi, jnp.pi], N_smp, cen=0., lev=100, b=0.05)\n",
        "# create the hilbert transform function based on the sampling\n",
        "hptrans = hilb_perid_cubic(x_smp)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 4), dpi=100)\n",
        "ax = plt.subplot(111)\n",
        "ax.plot(x_smp, random.uniform(keys[0], x_smp.shape), 'kx', linewidth=1, label='col.point')\n",
        "plt.show()\n",
        "\n",
        "# prepare the collocation points to evaluate equation gradient\n",
        "dataf, weighf, F0 = data_func_create(N_fix, N_col, coeff)\n",
        "# sampling the data\n",
        "data_z = dataf(keys[0], F0)\n",
        "weigh = weighf(wp=0.1)\n",
        "data = dwcombine(data_z, weigh)\n",
        "\n",
        "# initialize the weights and biases of the network\n",
        "trained_params = stwv_init_MLP(keys[1], n_hl, n_unit)\n",
        "\n",
        "# read one group of collocation points\n",
        "x_c = data['col'][0]\n",
        "x_d = data['grad'][0]\n",
        "\n",
        "plot = 1\n",
        "if plot == 1:\n",
        "    fig = plt.figure(figsize=(10, 4), dpi=100)\n",
        "    ax = plt.subplot(111)\n",
        "    ax.plot(x_c, random.uniform(keys[0], x_c.shape), 'kx', linewidth=1, label='col.point')\n",
        "    plt.show()\n",
        "\n",
        "# create the solution function\n",
        "predf = stwv_pred_create(lmt)\n",
        "# calculate the loss function\n",
        "stwv_loss = loss_create(predf, cond, lamb0, lw, ew)\n",
        "# update the loss reference based on the real loss\n",
        "stwv_loss.ref = stwv_loss(trained_params, data)[0]\n",
        "print(stwv_loss.ref)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrTLgbU0F0Gm",
        "outputId": "aabfc12c-3a9d-4211-dcdd-df11750c964e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Step: 50 | Loss: 5.2731e-04 | Loss_d: 3.1928e-05 | Loss_e: 4.9538e-03 | lamb: 1.080000 | Dp: 9.1385e-01\n",
            "Step: 100 | Loss: 2.8055e-04 | Loss_d: 1.1876e-05 | Loss_e: 2.6867e-03 | lamb: 1.080000 | Dp: 8.2680e-01\n",
            "Step: 150 | Loss: 1.8143e-04 | Loss_d: 5.8880e-06 | Loss_e: 1.7554e-03 | lamb: 1.080000 | Dp: 7.4805e-01\n",
            "Step: 200 | Loss: 1.2897e-04 | Loss_d: 3.2058e-06 | Loss_e: 1.2576e-03 | lamb: 1.080000 | Dp: 6.7679e-01\n",
            "Step: 250 | Loss: 1.0132e-04 | Loss_d: 1.9377e-06 | Loss_e: 9.9378e-04 | lamb: 1.080000 | Dp: 6.1233e-01\n",
            "Step: 300 | Loss: 8.1050e-05 | Loss_d: 1.2295e-06 | Loss_e: 7.9820e-04 | lamb: 1.080000 | Dp: 5.5400e-01\n",
            "Step: 350 | Loss: 6.4613e-05 | Loss_d: 7.8578e-07 | Loss_e: 6.3827e-04 | lamb: 1.080000 | Dp: 5.0123e-01\n",
            "Step: 400 | Loss: 5.1802e-05 | Loss_d: 4.8530e-07 | Loss_e: 5.1317e-04 | lamb: 1.080000 | Dp: 4.5349e-01\n",
            "Step: 450 | Loss: 4.2504e-05 | Loss_d: 2.9030e-07 | Loss_e: 4.2214e-04 | lamb: 1.080000 | Dp: 4.1029e-01\n",
            "Step: 500 | Loss: 3.6142e-05 | Loss_d: 1.7318e-07 | Loss_e: 3.5969e-04 | lamb: 1.080000 | Dp: 3.7121e-01\n",
            "Step: 550 | Loss: 3.1863e-05 | Loss_d: 1.0249e-07 | Loss_e: 3.1760e-04 | lamb: 1.080000 | Dp: 3.3585e-01\n",
            "Step: 600 | Loss: 2.8908e-05 | Loss_d: 6.0591e-08 | Loss_e: 2.8848e-04 | lamb: 1.080000 | Dp: 3.0386e-01\n",
            "Step: 650 | Loss: 2.6784e-05 | Loss_d: 3.5657e-08 | Loss_e: 2.6749e-04 | lamb: 1.080000 | Dp: 2.7492e-01\n",
            "Step: 700 | Loss: 2.5208e-05 | Loss_d: 2.0706e-08 | Loss_e: 2.5187e-04 | lamb: 1.080000 | Dp: 2.4873e-01\n",
            "Step: 750 | Loss: 2.4010e-05 | Loss_d: 1.1651e-08 | Loss_e: 2.3998e-04 | lamb: 1.080000 | Dp: 2.2504e-01\n",
            "Step: 800 | Loss: 2.3075e-05 | Loss_d: 6.0507e-09 | Loss_e: 2.3069e-04 | lamb: 1.080000 | Dp: 2.0360e-01\n",
            "Step: 850 | Loss: 2.2324e-05 | Loss_d: 2.9368e-09 | Loss_e: 2.2321e-04 | lamb: 1.080000 | Dp: 1.8421e-01\n",
            "Step: 900 | Loss: 2.1706e-05 | Loss_d: 1.1190e-09 | Loss_e: 2.1705e-04 | lamb: 1.080000 | Dp: 1.6666e-01\n",
            "Step: 950 | Loss: 2.1192e-05 | Loss_d: 2.4921e-10 | Loss_e: 2.1192e-04 | lamb: 1.080000 | Dp: 1.5079e-01\n",
            "Step: 1000 | Loss: 2.0761e-05 | Loss_d: 5.4546e-13 | Loss_e: 2.0761e-04 | lamb: 1.080000 | Dp: 1.3642e-01\n",
            "Step: 1050 | Loss: 3.4929e-05 | Loss_d: 0.0000e+00 | Loss_e: 3.4929e-04 | lamb: 1.080000 | Dp: 1.2343e-01\n",
            "Step: 1100 | Loss: 3.3834e-05 | Loss_d: 0.0000e+00 | Loss_e: 3.3834e-04 | lamb: 1.080000 | Dp: 1.1167e-01\n",
            "Step: 1150 | Loss: 3.2729e-05 | Loss_d: 0.0000e+00 | Loss_e: 3.2729e-04 | lamb: 1.080000 | Dp: 1.0103e-01\n",
            "Step: 1200 | Loss: 3.1602e-05 | Loss_d: 0.0000e+00 | Loss_e: 3.1602e-04 | lamb: 1.080000 | Dp: 9.1411e-02\n",
            "Step: 1250 | Loss: 3.0442e-05 | Loss_d: 0.0000e+00 | Loss_e: 3.0442e-04 | lamb: 1.080000 | Dp: 8.2704e-02\n",
            "Step: 1300 | Loss: 2.9240e-05 | Loss_d: 0.0000e+00 | Loss_e: 2.9240e-04 | lamb: 1.080000 | Dp: 7.4826e-02\n",
            "Step: 1350 | Loss: 2.7990e-05 | Loss_d: 0.0000e+00 | Loss_e: 2.7990e-04 | lamb: 1.080000 | Dp: 6.7698e-02\n",
            "Step: 1400 | Loss: 2.6694e-05 | Loss_d: 0.0000e+00 | Loss_e: 2.6694e-04 | lamb: 1.080000 | Dp: 6.1250e-02\n",
            "Step: 1450 | Loss: 2.5365e-05 | Loss_d: 0.0000e+00 | Loss_e: 2.5365e-04 | lamb: 1.080000 | Dp: 5.5416e-02\n",
            "Step: 1500 | Loss: 2.4028e-05 | Loss_d: 0.0000e+00 | Loss_e: 2.4028e-04 | lamb: 1.080000 | Dp: 5.0137e-02\n",
            "Step: 1550 | Loss: 2.2708e-05 | Loss_d: 0.0000e+00 | Loss_e: 2.2708e-04 | lamb: 1.080000 | Dp: 4.5361e-02\n",
            "Step: 1600 | Loss: 2.1408e-05 | Loss_d: 0.0000e+00 | Loss_e: 2.1408e-04 | lamb: 1.080000 | Dp: 4.1041e-02\n",
            "Step: 1650 | Loss: 2.0103e-05 | Loss_d: 0.0000e+00 | Loss_e: 2.0103e-04 | lamb: 1.080000 | Dp: 3.7131e-02\n",
            "Step: 1700 | Loss: 1.8782e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.8782e-04 | lamb: 1.080000 | Dp: 3.3594e-02\n",
            "Step: 1750 | Loss: 1.7491e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.7491e-04 | lamb: 1.080000 | Dp: 3.0394e-02\n",
            "Step: 1800 | Loss: 1.6309e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.6309e-04 | lamb: 1.080000 | Dp: 2.7499e-02\n",
            "Step: 1850 | Loss: 1.5277e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.5277e-04 | lamb: 1.080000 | Dp: 2.4880e-02\n",
            "Step: 1900 | Loss: 1.4367e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.4367e-04 | lamb: 1.080000 | Dp: 2.2510e-02\n",
            "Step: 1950 | Loss: 1.3538e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.3538e-04 | lamb: 1.080000 | Dp: 2.0366e-02\n",
            "Step: 2000 | Loss: 1.2763e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.2763e-04 | lamb: 1.080000 | Dp: 1.8426e-02\n",
            "Step: 2050 | Loss: 1.3267e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.3267e-04 | lamb: 1.080000 | Dp: 1.6671e-02\n",
            "Step: 2100 | Loss: 1.1864e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.1864e-04 | lamb: 1.080000 | Dp: 1.5083e-02\n",
            "Step: 2150 | Loss: 1.0566e-05 | Loss_d: 0.0000e+00 | Loss_e: 1.0566e-04 | lamb: 1.080000 | Dp: 1.3646e-02\n",
            "Step: 2200 | Loss: 9.4998e-06 | Loss_d: 0.0000e+00 | Loss_e: 9.4998e-05 | lamb: 1.080000 | Dp: 1.2346e-02\n",
            "Step: 2250 | Loss: 8.6704e-06 | Loss_d: 0.0000e+00 | Loss_e: 8.6704e-05 | lamb: 1.080000 | Dp: 1.1170e-02\n",
            "Step: 2300 | Loss: 7.9965e-06 | Loss_d: 0.0000e+00 | Loss_e: 7.9965e-05 | lamb: 1.080000 | Dp: 1.0106e-02\n",
            "Step: 2350 | Loss: 7.4217e-06 | Loss_d: 0.0000e+00 | Loss_e: 7.4217e-05 | lamb: 1.080000 | Dp: 9.1436e-03\n",
            "Step: 2400 | Loss: 6.9160e-06 | Loss_d: 0.0000e+00 | Loss_e: 6.9160e-05 | lamb: 1.080000 | Dp: 8.2727e-03\n",
            "Step: 2450 | Loss: 6.4580e-06 | Loss_d: 0.0000e+00 | Loss_e: 6.4580e-05 | lamb: 1.080000 | Dp: 7.4847e-03\n",
            "Step: 2500 | Loss: 6.0298e-06 | Loss_d: 0.0000e+00 | Loss_e: 6.0298e-05 | lamb: 1.080000 | Dp: 6.7717e-03\n",
            "Step: 2550 | Loss: 5.6183e-06 | Loss_d: 0.0000e+00 | Loss_e: 5.6183e-05 | lamb: 1.080000 | Dp: 6.1267e-03\n",
            "Step: 2600 | Loss: 5.2158e-06 | Loss_d: 0.0000e+00 | Loss_e: 5.2158e-05 | lamb: 1.080000 | Dp: 5.5431e-03\n",
            "Step: 2650 | Loss: 4.8192e-06 | Loss_d: 0.0000e+00 | Loss_e: 4.8192e-05 | lamb: 1.080000 | Dp: 5.0151e-03\n",
            "Step: 2700 | Loss: 4.4287e-06 | Loss_d: 0.0000e+00 | Loss_e: 4.4287e-05 | lamb: 1.080000 | Dp: 4.5374e-03\n",
            "Step: 2750 | Loss: 4.0470e-06 | Loss_d: 0.0000e+00 | Loss_e: 4.0470e-05 | lamb: 1.080000 | Dp: 4.1052e-03\n",
            "Step: 2800 | Loss: 3.6779e-06 | Loss_d: 0.0000e+00 | Loss_e: 3.6779e-05 | lamb: 1.080000 | Dp: 3.7142e-03\n",
            "Step: 2850 | Loss: 3.3273e-06 | Loss_d: 0.0000e+00 | Loss_e: 3.3273e-05 | lamb: 1.080000 | Dp: 3.3604e-03\n",
            "Step: 2900 | Loss: 3.0018e-06 | Loss_d: 0.0000e+00 | Loss_e: 3.0018e-05 | lamb: 1.080000 | Dp: 3.0403e-03\n",
            "Step: 2950 | Loss: 2.7083e-06 | Loss_d: 0.0000e+00 | Loss_e: 2.7083e-05 | lamb: 1.080000 | Dp: 2.7507e-03\n",
            "Step: 3000 | Loss: 2.4517e-06 | Loss_d: 0.0000e+00 | Loss_e: 2.4517e-05 | lamb: 1.080000 | Dp: 2.4887e-03\n",
            "Step: 3050 | Loss: 2.5406e-06 | Loss_d: 0.0000e+00 | Loss_e: 2.5406e-05 | lamb: 1.080000 | Dp: 2.2516e-03\n",
            "Step: 3100 | Loss: 2.2571e-06 | Loss_d: 0.0000e+00 | Loss_e: 2.2571e-05 | lamb: 1.080000 | Dp: 2.0372e-03\n",
            "Step: 3150 | Loss: 1.9895e-06 | Loss_d: 0.0000e+00 | Loss_e: 1.9895e-05 | lamb: 1.080000 | Dp: 1.8431e-03\n",
            "Step: 3200 | Loss: 1.7499e-06 | Loss_d: 0.0000e+00 | Loss_e: 1.7499e-05 | lamb: 1.080000 | Dp: 1.6676e-03\n",
            "Step: 3250 | Loss: 1.5480e-06 | Loss_d: 0.0000e+00 | Loss_e: 1.5480e-05 | lamb: 1.080000 | Dp: 1.5087e-03\n",
            "Step: 3300 | Loss: 1.3866e-06 | Loss_d: 0.0000e+00 | Loss_e: 1.3866e-05 | lamb: 1.080000 | Dp: 1.3650e-03\n",
            "Step: 3350 | Loss: 1.2555e-06 | Loss_d: 0.0000e+00 | Loss_e: 1.2555e-05 | lamb: 1.080000 | Dp: 1.2350e-03\n",
            "Step: 3400 | Loss: 1.1399e-06 | Loss_d: 0.0000e+00 | Loss_e: 1.1399e-05 | lamb: 1.080000 | Dp: 1.1173e-03\n",
            "Step: 3450 | Loss: 1.0309e-06 | Loss_d: 0.0000e+00 | Loss_e: 1.0309e-05 | lamb: 1.080000 | Dp: 1.0109e-03\n",
            "Step: 3500 | Loss: 9.2477e-07 | Loss_d: 0.0000e+00 | Loss_e: 9.2477e-06 | lamb: 1.080000 | Dp: 9.1462e-04\n",
            "Step: 3550 | Loss: 8.2101e-07 | Loss_d: 0.0000e+00 | Loss_e: 8.2101e-06 | lamb: 1.080000 | Dp: 8.2750e-04\n",
            "Step: 3600 | Loss: 7.2118e-07 | Loss_d: 0.0000e+00 | Loss_e: 7.2118e-06 | lamb: 1.080000 | Dp: 7.4868e-04\n",
            "Step: 3650 | Loss: 6.2822e-07 | Loss_d: 0.0000e+00 | Loss_e: 6.2822e-06 | lamb: 1.080000 | Dp: 6.7737e-04\n",
            "Step: 3700 | Loss: 5.4524e-07 | Loss_d: 0.0000e+00 | Loss_e: 5.4524e-06 | lamb: 1.080000 | Dp: 6.1284e-04\n",
            "Step: 3750 | Loss: 4.7445e-07 | Loss_d: 0.0000e+00 | Loss_e: 4.7445e-06 | lamb: 1.080000 | Dp: 5.5447e-04\n",
            "Step: 3800 | Loss: 4.1606e-07 | Loss_d: 0.0000e+00 | Loss_e: 4.1606e-06 | lamb: 1.080000 | Dp: 5.0165e-04\n",
            "Step: 3850 | Loss: 3.6823e-07 | Loss_d: 0.0000e+00 | Loss_e: 3.6823e-06 | lamb: 1.080000 | Dp: 4.5387e-04\n",
            "Step: 3900 | Loss: 3.2813e-07 | Loss_d: 0.0000e+00 | Loss_e: 3.2813e-06 | lamb: 1.080000 | Dp: 4.1064e-04\n",
            "Step: 3950 | Loss: 2.9320e-07 | Loss_d: 0.0000e+00 | Loss_e: 2.9320e-06 | lamb: 1.080000 | Dp: 3.7152e-04\n",
            "Step: 4000 | Loss: 2.6175e-07 | Loss_d: 0.0000e+00 | Loss_e: 2.6175e-06 | lamb: 1.080000 | Dp: 3.3613e-04\n",
            "Step: 4050 | Loss: 1.9220e-07 | Loss_d: 0.0000e+00 | Loss_e: 1.9220e-06 | lamb: 1.080000 | Dp: 3.0412e-04\n",
            "Step: 4100 | Loss: 1.6612e-07 | Loss_d: 0.0000e+00 | Loss_e: 1.6612e-06 | lamb: 1.080000 | Dp: 2.7515e-04\n",
            "Step: 4150 | Loss: 1.4445e-07 | Loss_d: 0.0000e+00 | Loss_e: 1.4445e-06 | lamb: 1.080000 | Dp: 2.4894e-04\n",
            "Step: 4200 | Loss: 1.2708e-07 | Loss_d: 0.0000e+00 | Loss_e: 1.2708e-06 | lamb: 1.080000 | Dp: 2.2523e-04\n",
            "Step: 4250 | Loss: 1.1264e-07 | Loss_d: 0.0000e+00 | Loss_e: 1.1264e-06 | lamb: 1.080000 | Dp: 2.0377e-04\n",
            "Step: 4300 | Loss: 1.0019e-07 | Loss_d: 0.0000e+00 | Loss_e: 1.0019e-06 | lamb: 1.080000 | Dp: 1.8436e-04\n",
            "Step: 4350 | Loss: 8.9066e-08 | Loss_d: 0.0000e+00 | Loss_e: 8.9066e-07 | lamb: 1.080000 | Dp: 1.6680e-04\n",
            "Step: 4400 | Loss: 7.8841e-08 | Loss_d: 0.0000e+00 | Loss_e: 7.8841e-07 | lamb: 1.080000 | Dp: 1.5091e-04\n",
            "Step: 4450 | Loss: 6.9260e-08 | Loss_d: 0.0000e+00 | Loss_e: 6.9260e-07 | lamb: 1.080000 | Dp: 1.3654e-04\n",
            "Step: 4500 | Loss: 6.0160e-08 | Loss_d: 0.0000e+00 | Loss_e: 6.0160e-07 | lamb: 1.080000 | Dp: 1.2353e-04\n",
            "Step: 4550 | Loss: 5.1489e-08 | Loss_d: 0.0000e+00 | Loss_e: 5.1489e-07 | lamb: 1.080000 | Dp: 1.1177e-04\n",
            "Step: 4600 | Loss: 4.3434e-08 | Loss_d: 0.0000e+00 | Loss_e: 4.3434e-07 | lamb: 1.080000 | Dp: 1.0112e-04\n",
            "Step: 4650 | Loss: 3.6413e-08 | Loss_d: 0.0000e+00 | Loss_e: 3.6413e-07 | lamb: 1.080000 | Dp: 9.1488e-05\n",
            "Step: 4700 | Loss: 3.0699e-08 | Loss_d: 0.0000e+00 | Loss_e: 3.0699e-07 | lamb: 1.080000 | Dp: 8.2774e-05\n",
            "Step: 4750 | Loss: 2.6231e-08 | Loss_d: 0.0000e+00 | Loss_e: 2.6231e-07 | lamb: 1.080000 | Dp: 7.4889e-05\n",
            "Step: 4800 | Loss: 2.2777e-08 | Loss_d: 0.0000e+00 | Loss_e: 2.2777e-07 | lamb: 1.080000 | Dp: 6.7756e-05\n",
            "Step: 4850 | Loss: 2.0062e-08 | Loss_d: 0.0000e+00 | Loss_e: 2.0062e-07 | lamb: 1.080000 | Dp: 6.1302e-05\n",
            "Step: 4900 | Loss: 1.7854e-08 | Loss_d: 0.0000e+00 | Loss_e: 1.7854e-07 | lamb: 1.080000 | Dp: 5.5463e-05\n",
            "Step: 4950 | Loss: 1.5997e-08 | Loss_d: 0.0000e+00 | Loss_e: 1.5997e-07 | lamb: 1.080000 | Dp: 5.0180e-05\n",
            "Step: 5000 | Loss: 1.4401e-08 | Loss_d: 0.0000e+00 | Loss_e: 1.4401e-07 | lamb: 1.080000 | Dp: 4.5400e-05\n",
            "Step: 5050 | Loss: 1.4440e-08 | Loss_d: 0.0000e+00 | Loss_e: 1.4440e-07 | lamb: 1.080000 | Dp: 4.1075e-05\n",
            "Step: 5100 | Loss: 1.2023e-08 | Loss_d: 0.0000e+00 | Loss_e: 1.2023e-07 | lamb: 1.080000 | Dp: 3.7163e-05\n",
            "Step: 5150 | Loss: 1.0200e-08 | Loss_d: 0.0000e+00 | Loss_e: 1.0200e-07 | lamb: 1.080000 | Dp: 3.3623e-05\n",
            "Step: 5200 | Loss: 8.8051e-09 | Loss_d: 0.0000e+00 | Loss_e: 8.8051e-08 | lamb: 1.080000 | Dp: 3.0420e-05\n",
            "Step: 5250 | Loss: 7.7050e-09 | Loss_d: 0.0000e+00 | Loss_e: 7.7050e-08 | lamb: 1.080000 | Dp: 2.7523e-05\n",
            "Step: 5300 | Loss: 6.8111e-09 | Loss_d: 0.0000e+00 | Loss_e: 6.8111e-08 | lamb: 1.080000 | Dp: 2.4901e-05\n",
            "Step: 5350 | Loss: 6.0655e-09 | Loss_d: 0.0000e+00 | Loss_e: 6.0655e-08 | lamb: 1.080000 | Dp: 2.2529e-05\n",
            "Step: 5400 | Loss: 5.4311e-09 | Loss_d: 0.0000e+00 | Loss_e: 5.4311e-08 | lamb: 1.080000 | Dp: 2.0383e-05\n",
            "Step: 5450 | Loss: 4.8830e-09 | Loss_d: 0.0000e+00 | Loss_e: 4.8830e-08 | lamb: 1.080000 | Dp: 1.8442e-05\n",
            "Step: 5500 | Loss: 4.4022e-09 | Loss_d: 0.0000e+00 | Loss_e: 4.4022e-08 | lamb: 1.080000 | Dp: 1.6685e-05\n",
            "Step: 5550 | Loss: 3.9745e-09 | Loss_d: 0.0000e+00 | Loss_e: 3.9745e-08 | lamb: 1.080000 | Dp: 1.5096e-05\n",
            "Step: 5600 | Loss: 3.5897e-09 | Loss_d: 0.0000e+00 | Loss_e: 3.5897e-08 | lamb: 1.080000 | Dp: 1.3658e-05\n",
            "Step: 5650 | Loss: 3.2407e-09 | Loss_d: 0.0000e+00 | Loss_e: 3.2407e-08 | lamb: 1.080000 | Dp: 1.2357e-05\n",
            "Step: 5700 | Loss: 2.9225e-09 | Loss_d: 0.0000e+00 | Loss_e: 2.9225e-08 | lamb: 1.080000 | Dp: 1.1180e-05\n",
            "Step: 5750 | Loss: 2.6321e-09 | Loss_d: 0.0000e+00 | Loss_e: 2.6321e-08 | lamb: 1.080000 | Dp: 1.0115e-05\n",
            "Step: 5800 | Loss: 2.3674e-09 | Loss_d: 0.0000e+00 | Loss_e: 2.3674e-08 | lamb: 1.080000 | Dp: 9.1514e-06\n",
            "Step: 5850 | Loss: 2.1265e-09 | Loss_d: 0.0000e+00 | Loss_e: 2.1265e-08 | lamb: 1.080000 | Dp: 8.2797e-06\n",
            "Step: 5900 | Loss: 1.9080e-09 | Loss_d: 0.0000e+00 | Loss_e: 1.9080e-08 | lamb: 1.080000 | Dp: 7.4910e-06\n",
            "Step: 5950 | Loss: 1.7104e-09 | Loss_d: 0.0000e+00 | Loss_e: 1.7104e-08 | lamb: 1.080000 | Dp: 6.7775e-06\n",
            "Step: 6000 | Loss: 1.5325e-09 | Loss_d: 0.0000e+00 | Loss_e: 1.5325e-08 | lamb: 1.080000 | Dp: 6.1319e-06\n",
            "Step: 6050 | Loss: 1.3376e-09 | Loss_d: 0.0000e+00 | Loss_e: 1.3376e-08 | lamb: 1.080000 | Dp: 5.5478e-06\n",
            "Step: 6100 | Loss: 1.1557e-09 | Loss_d: 0.0000e+00 | Loss_e: 1.1557e-08 | lamb: 1.080000 | Dp: 5.0194e-06\n",
            "Step: 6150 | Loss: 9.9596e-10 | Loss_d: 0.0000e+00 | Loss_e: 9.9596e-09 | lamb: 1.080000 | Dp: 4.5413e-06\n",
            "Step: 6200 | Loss: 8.5064e-10 | Loss_d: 0.0000e+00 | Loss_e: 8.5064e-09 | lamb: 1.080000 | Dp: 4.1087e-06\n",
            "Step: 6250 | Loss: 7.2471e-10 | Loss_d: 0.0000e+00 | Loss_e: 7.2471e-09 | lamb: 1.080000 | Dp: 3.7173e-06\n",
            "Step: 6300 | Loss: 6.1637e-10 | Loss_d: 0.0000e+00 | Loss_e: 6.1637e-09 | lamb: 1.080000 | Dp: 3.3632e-06\n",
            "Step: 6350 | Loss: 5.2473e-10 | Loss_d: 0.0000e+00 | Loss_e: 5.2473e-09 | lamb: 1.080000 | Dp: 3.0429e-06\n",
            "Step: 6400 | Loss: 4.4892e-10 | Loss_d: 0.0000e+00 | Loss_e: 4.4892e-09 | lamb: 1.080000 | Dp: 2.7530e-06\n",
            "Step: 6450 | Loss: 3.8758e-10 | Loss_d: 0.0000e+00 | Loss_e: 3.8758e-09 | lamb: 1.080000 | Dp: 2.4908e-06\n",
            "Step: 6500 | Loss: 3.3891e-10 | Loss_d: 0.0000e+00 | Loss_e: 3.3891e-09 | lamb: 1.080000 | Dp: 2.2535e-06\n",
            "Step: 6550 | Loss: 3.0066e-10 | Loss_d: 0.0000e+00 | Loss_e: 3.0066e-09 | lamb: 1.080000 | Dp: 2.0389e-06\n",
            "Step: 6600 | Loss: 2.7051e-10 | Loss_d: 0.0000e+00 | Loss_e: 2.7051e-09 | lamb: 1.080000 | Dp: 1.8447e-06\n",
            "Step: 6650 | Loss: 2.4647e-10 | Loss_d: 0.0000e+00 | Loss_e: 2.4647e-09 | lamb: 1.080000 | Dp: 1.6690e-06\n",
            "Step: 6700 | Loss: 2.2677e-10 | Loss_d: 0.0000e+00 | Loss_e: 2.2677e-09 | lamb: 1.080000 | Dp: 1.5100e-06\n",
            "Step: 6750 | Loss: 2.1013e-10 | Loss_d: 0.0000e+00 | Loss_e: 2.1013e-09 | lamb: 1.080000 | Dp: 1.3662e-06\n",
            "Step: 6800 | Loss: 1.9572e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.9572e-09 | lamb: 1.080000 | Dp: 1.2360e-06\n",
            "Step: 6850 | Loss: 1.8298e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.8298e-09 | lamb: 1.080000 | Dp: 1.1183e-06\n",
            "Step: 6900 | Loss: 1.7159e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.7159e-09 | lamb: 1.080000 | Dp: 1.0118e-06\n",
            "Step: 6950 | Loss: 1.6128e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.6128e-09 | lamb: 1.080000 | Dp: 9.1540e-07\n",
            "Step: 7000 | Loss: 1.5185e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.5185e-09 | lamb: 1.080000 | Dp: 8.2820e-07\n",
            "Step: 7050 | Loss: 1.5700e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.5700e-09 | lamb: 1.080000 | Dp: 7.4931e-07\n",
            "Step: 7100 | Loss: 1.3475e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.3475e-09 | lamb: 1.080000 | Dp: 6.7794e-07\n",
            "Step: 7150 | Loss: 1.2445e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.2445e-09 | lamb: 1.080000 | Dp: 6.1336e-07\n",
            "Step: 7200 | Loss: 1.1568e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.1568e-09 | lamb: 1.080000 | Dp: 5.5494e-07\n",
            "Step: 7250 | Loss: 1.0731e-10 | Loss_d: 0.0000e+00 | Loss_e: 1.0731e-09 | lamb: 1.080000 | Dp: 5.0208e-07\n",
            "Step: 7300 | Loss: 9.9221e-11 | Loss_d: 0.0000e+00 | Loss_e: 9.9221e-10 | lamb: 1.080000 | Dp: 4.5425e-07\n",
            "Step: 7350 | Loss: 9.1457e-11 | Loss_d: 0.0000e+00 | Loss_e: 9.1457e-10 | lamb: 1.080000 | Dp: 4.1098e-07\n",
            "Step: 7400 | Loss: 8.4029e-11 | Loss_d: 0.0000e+00 | Loss_e: 8.4029e-10 | lamb: 1.080000 | Dp: 3.7184e-07\n",
            "Step: 7450 | Loss: 7.6973e-11 | Loss_d: 0.0000e+00 | Loss_e: 7.6973e-10 | lamb: 1.080000 | Dp: 3.3642e-07\n",
            "Step: 7500 | Loss: 7.0323e-11 | Loss_d: 0.0000e+00 | Loss_e: 7.0323e-10 | lamb: 1.080000 | Dp: 3.0437e-07\n",
            "Step: 7550 | Loss: 6.4108e-11 | Loss_d: 0.0000e+00 | Loss_e: 6.4108e-10 | lamb: 1.080000 | Dp: 2.7538e-07\n",
            "Step: 7600 | Loss: 5.8340e-11 | Loss_d: 0.0000e+00 | Loss_e: 5.8340e-10 | lamb: 1.080000 | Dp: 2.4915e-07\n",
            "Step: 7650 | Loss: 5.3019e-11 | Loss_d: 0.0000e+00 | Loss_e: 5.3019e-10 | lamb: 1.080000 | Dp: 2.2542e-07\n",
            "Step: 7700 | Loss: 4.8140e-11 | Loss_d: 0.0000e+00 | Loss_e: 4.8140e-10 | lamb: 1.080000 | Dp: 2.0395e-07\n",
            "Step: 7750 | Loss: 4.3686e-11 | Loss_d: 0.0000e+00 | Loss_e: 4.3686e-10 | lamb: 1.080000 | Dp: 1.8452e-07\n",
            "Step: 7800 | Loss: 3.9629e-11 | Loss_d: 0.0000e+00 | Loss_e: 3.9629e-10 | lamb: 1.080000 | Dp: 1.6694e-07\n",
            "Step: 7850 | Loss: 3.5934e-11 | Loss_d: 0.0000e+00 | Loss_e: 3.5934e-10 | lamb: 1.080000 | Dp: 1.5104e-07\n",
            "Step: 7900 | Loss: 3.2574e-11 | Loss_d: 0.0000e+00 | Loss_e: 3.2574e-10 | lamb: 1.080000 | Dp: 1.3665e-07\n",
            "Step: 7950 | Loss: 2.9523e-11 | Loss_d: 0.0000e+00 | Loss_e: 2.9523e-10 | lamb: 1.080000 | Dp: 1.2364e-07\n",
            "Step: 8000 | Loss: 2.6759e-11 | Loss_d: 0.0000e+00 | Loss_e: 2.6759e-10 | lamb: 1.080000 | Dp: 1.1186e-07\n",
            "Step: 8050 | Loss: 3.2049e-11 | Loss_d: 0.0000e+00 | Loss_e: 3.2049e-10 | lamb: 1.080000 | Dp: 1.0121e-07\n",
            "Step: 8100 | Loss: 2.3526e-11 | Loss_d: 0.0000e+00 | Loss_e: 2.3526e-10 | lamb: 1.080000 | Dp: 9.1565e-08\n",
            "Step: 8150 | Loss: 2.0023e-11 | Loss_d: 0.0000e+00 | Loss_e: 2.0023e-10 | lamb: 1.080000 | Dp: 8.2844e-08\n",
            "Step: 8200 | Loss: 1.7751e-11 | Loss_d: 0.0000e+00 | Loss_e: 1.7751e-10 | lamb: 1.080000 | Dp: 7.4952e-08\n",
            "Step: 8250 | Loss: 1.5807e-11 | Loss_d: 0.0000e+00 | Loss_e: 1.5807e-10 | lamb: 1.080000 | Dp: 6.7813e-08\n",
            "Step: 8300 | Loss: 1.4140e-11 | Loss_d: 0.0000e+00 | Loss_e: 1.4140e-10 | lamb: 1.080000 | Dp: 6.1354e-08\n",
            "Step: 8350 | Loss: 1.2694e-11 | Loss_d: 0.0000e+00 | Loss_e: 1.2694e-10 | lamb: 1.080000 | Dp: 5.5509e-08\n",
            "Step: 8400 | Loss: 1.1414e-11 | Loss_d: 0.0000e+00 | Loss_e: 1.1414e-10 | lamb: 1.080000 | Dp: 5.0222e-08\n",
            "Step: 8450 | Loss: 1.0264e-11 | Loss_d: 0.0000e+00 | Loss_e: 1.0264e-10 | lamb: 1.080000 | Dp: 4.5438e-08\n",
            "Step: 8500 | Loss: 9.2191e-12 | Loss_d: 0.0000e+00 | Loss_e: 9.2191e-11 | lamb: 1.080000 | Dp: 4.1110e-08\n",
            "Step: 8550 | Loss: 8.2643e-12 | Loss_d: 0.0000e+00 | Loss_e: 8.2643e-11 | lamb: 1.080000 | Dp: 3.7194e-08\n",
            "Step: 8600 | Loss: 7.3931e-12 | Loss_d: 0.0000e+00 | Loss_e: 7.3931e-11 | lamb: 1.080000 | Dp: 3.3651e-08\n",
            "Step: 8650 | Loss: 6.6013e-12 | Loss_d: 0.0000e+00 | Loss_e: 6.6013e-11 | lamb: 1.080000 | Dp: 3.0446e-08\n",
            "Step: 8700 | Loss: 5.8840e-12 | Loss_d: 0.0000e+00 | Loss_e: 5.8840e-11 | lamb: 1.080000 | Dp: 2.7546e-08\n",
            "Step: 8750 | Loss: 5.2352e-12 | Loss_d: 0.0000e+00 | Loss_e: 5.2352e-11 | lamb: 1.080000 | Dp: 2.4922e-08\n",
            "Step: 8800 | Loss: 4.6452e-12 | Loss_d: 0.0000e+00 | Loss_e: 4.6452e-11 | lamb: 1.080000 | Dp: 2.2548e-08\n",
            "Step: 8850 | Loss: 4.1070e-12 | Loss_d: 0.0000e+00 | Loss_e: 4.1070e-11 | lamb: 1.080000 | Dp: 2.0400e-08\n",
            "Step: 8900 | Loss: 3.6164e-12 | Loss_d: 0.0000e+00 | Loss_e: 3.6164e-11 | lamb: 1.080000 | Dp: 1.8457e-08\n",
            "Step: 8950 | Loss: 3.1696e-12 | Loss_d: 0.0000e+00 | Loss_e: 3.1696e-11 | lamb: 1.080000 | Dp: 1.6699e-08\n",
            "Step: 9000 | Loss: 2.7658e-12 | Loss_d: 0.0000e+00 | Loss_e: 2.7658e-11 | lamb: 1.080000 | Dp: 1.5108e-08\n",
            "Step: 9050 | Loss: 4.1790e-12 | Loss_d: 0.0000e+00 | Loss_e: 4.1790e-11 | lamb: 1.080000 | Dp: 1.3669e-08\n",
            "Step: 9100 | Loss: 3.5678e-12 | Loss_d: 0.0000e+00 | Loss_e: 3.5678e-11 | lamb: 1.080000 | Dp: 1.2367e-08\n",
            "Step: 9150 | Loss: 2.9728e-12 | Loss_d: 0.0000e+00 | Loss_e: 2.9728e-11 | lamb: 1.080000 | Dp: 1.1189e-08\n",
            "Step: 9200 | Loss: 2.4462e-12 | Loss_d: 0.0000e+00 | Loss_e: 2.4462e-11 | lamb: 1.080000 | Dp: 1.0123e-08\n",
            "Step: 9250 | Loss: 2.0104e-12 | Loss_d: 0.0000e+00 | Loss_e: 2.0104e-11 | lamb: 1.080000 | Dp: 9.1591e-09\n",
            "Step: 9300 | Loss: 1.6548e-12 | Loss_d: 0.0000e+00 | Loss_e: 1.6548e-11 | lamb: 1.080000 | Dp: 8.2867e-09\n",
            "Step: 9350 | Loss: 1.3691e-12 | Loss_d: 0.0000e+00 | Loss_e: 1.3691e-11 | lamb: 1.080000 | Dp: 7.4974e-09\n",
            "Step: 9400 | Loss: 1.1428e-12 | Loss_d: 0.0000e+00 | Loss_e: 1.1428e-11 | lamb: 1.080000 | Dp: 6.7832e-09\n",
            "Step: 9450 | Loss: 9.6567e-13 | Loss_d: 0.0000e+00 | Loss_e: 9.6567e-12 | lamb: 1.080000 | Dp: 6.1371e-09\n",
            "Step: 9500 | Loss: 8.2884e-13 | Loss_d: 0.0000e+00 | Loss_e: 8.2884e-12 | lamb: 1.080000 | Dp: 5.5525e-09\n",
            "Step: 9550 | Loss: 7.2446e-13 | Loss_d: 0.0000e+00 | Loss_e: 7.2446e-12 | lamb: 1.080000 | Dp: 5.0236e-09\n",
            "Step: 9600 | Loss: 6.4571e-13 | Loss_d: 0.0000e+00 | Loss_e: 6.4571e-12 | lamb: 1.080000 | Dp: 4.5451e-09\n",
            "Step: 9650 | Loss: 5.8666e-13 | Loss_d: 0.0000e+00 | Loss_e: 5.8666e-12 | lamb: 1.080000 | Dp: 4.1122e-09\n",
            "Step: 9700 | Loss: 5.4252e-13 | Loss_d: 0.0000e+00 | Loss_e: 5.4252e-12 | lamb: 1.080000 | Dp: 3.7205e-09\n",
            "Step: 9750 | Loss: 5.0927e-13 | Loss_d: 0.0000e+00 | Loss_e: 5.0927e-12 | lamb: 1.080000 | Dp: 3.3661e-09\n",
            "Step: 9800 | Loss: 4.8372e-13 | Loss_d: 0.0000e+00 | Loss_e: 4.8372e-12 | lamb: 1.080000 | Dp: 3.0455e-09\n",
            "Step: 9850 | Loss: 4.6341e-13 | Loss_d: 0.0000e+00 | Loss_e: 4.6341e-12 | lamb: 1.080000 | Dp: 2.7554e-09\n",
            "Step: 9900 | Loss: 4.4660e-13 | Loss_d: 0.0000e+00 | Loss_e: 4.4660e-12 | lamb: 1.080000 | Dp: 2.4929e-09\n",
            "Step: 9950 | Loss: 4.3179e-13 | Loss_d: 0.0000e+00 | Loss_e: 4.3179e-12 | lamb: 1.080000 | Dp: 2.2554e-09\n",
            "Step: 10000 | Loss: 4.1816e-13 | Loss_d: 0.0000e+00 | Loss_e: 4.1816e-12 | lamb: 1.080000 | Dp: 2.0406e-09\n",
            "Step: 10050 | Loss: 4.1143e-13 | Loss_d: 0.0000e+00 | Loss_e: 4.1143e-12 | lamb: 1.080000 | Dp: 1.8462e-09\n",
            "Step: 10100 | Loss: 3.7250e-13 | Loss_d: 0.0000e+00 | Loss_e: 3.7250e-12 | lamb: 1.080000 | Dp: 1.6704e-09\n",
            "Step: 10150 | Loss: 3.3411e-13 | Loss_d: 0.0000e+00 | Loss_e: 3.3411e-12 | lamb: 1.080000 | Dp: 1.5113e-09\n",
            "Step: 10200 | Loss: 3.0289e-13 | Loss_d: 0.0000e+00 | Loss_e: 3.0289e-12 | lamb: 1.080000 | Dp: 1.3673e-09\n",
            "Step: 10250 | Loss: 2.7752e-13 | Loss_d: 0.0000e+00 | Loss_e: 2.7752e-12 | lamb: 1.080000 | Dp: 1.2371e-09\n",
            "Step: 10300 | Loss: 2.5651e-13 | Loss_d: 0.0000e+00 | Loss_e: 2.5651e-12 | lamb: 1.080000 | Dp: 1.1192e-09\n",
            "Step: 10350 | Loss: 2.3871e-13 | Loss_d: 0.0000e+00 | Loss_e: 2.3871e-12 | lamb: 1.080000 | Dp: 1.0126e-09\n",
            "Step: 10400 | Loss: 2.2330e-13 | Loss_d: 0.0000e+00 | Loss_e: 2.2330e-12 | lamb: 1.080000 | Dp: 9.1617e-10\n",
            "Step: 10450 | Loss: 2.0963e-13 | Loss_d: 0.0000e+00 | Loss_e: 2.0963e-12 | lamb: 1.080000 | Dp: 8.2890e-10\n",
            "Step: 10500 | Loss: 1.9713e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.9713e-12 | lamb: 1.080000 | Dp: 7.4995e-10\n",
            "Step: 10550 | Loss: 1.8561e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.8561e-12 | lamb: 1.080000 | Dp: 6.7851e-10\n",
            "Step: 10600 | Loss: 1.7493e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.7493e-12 | lamb: 1.080000 | Dp: 6.1388e-10\n",
            "Step: 10650 | Loss: 1.6501e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.6501e-12 | lamb: 1.080000 | Dp: 5.5541e-10\n",
            "Step: 10700 | Loss: 1.5580e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.5580e-12 | lamb: 1.080000 | Dp: 5.0250e-10\n",
            "Step: 10750 | Loss: 1.4730e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.4730e-12 | lamb: 1.080000 | Dp: 4.5464e-10\n",
            "Step: 10800 | Loss: 1.3948e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.3948e-12 | lamb: 1.080000 | Dp: 4.1133e-10\n",
            "Step: 10850 | Loss: 1.3229e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.3229e-12 | lamb: 1.080000 | Dp: 3.7215e-10\n",
            "Step: 10900 | Loss: 1.2561e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.2561e-12 | lamb: 1.080000 | Dp: 3.3670e-10\n",
            "Step: 10950 | Loss: 1.1939e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.1939e-12 | lamb: 1.080000 | Dp: 3.0463e-10\n",
            "Step: 11000 | Loss: 1.1357e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.1357e-12 | lamb: 1.080000 | Dp: 2.7561e-10\n",
            "Step: 11050 | Loss: 1.2873e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.2873e-12 | lamb: 1.080000 | Dp: 2.4936e-10\n",
            "Step: 11100 | Loss: 1.1805e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.1805e-12 | lamb: 1.080000 | Dp: 2.2561e-10\n",
            "Step: 11150 | Loss: 1.1234e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.1234e-12 | lamb: 1.080000 | Dp: 2.0412e-10\n",
            "Step: 11200 | Loss: 1.0559e-13 | Loss_d: 0.0000e+00 | Loss_e: 1.0559e-12 | lamb: 1.080000 | Dp: 1.8468e-10\n",
            "Step: 11250 | Loss: 9.9917e-14 | Loss_d: 0.0000e+00 | Loss_e: 9.9917e-13 | lamb: 1.080000 | Dp: 1.6708e-10\n",
            "Step: 11300 | Loss: 9.4347e-14 | Loss_d: 0.0000e+00 | Loss_e: 9.4347e-13 | lamb: 1.080000 | Dp: 1.5117e-10\n",
            "Step: 11350 | Loss: 8.9146e-14 | Loss_d: 0.0000e+00 | Loss_e: 8.9146e-13 | lamb: 1.080000 | Dp: 1.3677e-10\n",
            "Step: 11400 | Loss: 8.4370e-14 | Loss_d: 0.0000e+00 | Loss_e: 8.4370e-13 | lamb: 1.080000 | Dp: 1.2374e-10\n",
            "Step: 11450 | Loss: 7.9938e-14 | Loss_d: 0.0000e+00 | Loss_e: 7.9938e-13 | lamb: 1.080000 | Dp: 1.1196e-10\n",
            "Step: 11500 | Loss: 7.5846e-14 | Loss_d: 0.0000e+00 | Loss_e: 7.5846e-13 | lamb: 1.080000 | Dp: 1.0129e-10\n",
            "Step: 11550 | Loss: 7.2089e-14 | Loss_d: 0.0000e+00 | Loss_e: 7.2089e-13 | lamb: 1.080000 | Dp: 9.1643e-11\n",
            "Step: 11600 | Loss: 6.8597e-14 | Loss_d: 0.0000e+00 | Loss_e: 6.8597e-13 | lamb: 1.080000 | Dp: 8.2914e-11\n",
            "Step: 11650 | Loss: 6.5352e-14 | Loss_d: 0.0000e+00 | Loss_e: 6.5352e-13 | lamb: 1.080000 | Dp: 7.5016e-11\n",
            "Step: 11700 | Loss: 6.2324e-14 | Loss_d: 0.0000e+00 | Loss_e: 6.2324e-13 | lamb: 1.080000 | Dp: 6.7870e-11\n",
            "Step: 11750 | Loss: 5.9478e-14 | Loss_d: 0.0000e+00 | Loss_e: 5.9478e-13 | lamb: 1.080000 | Dp: 6.1406e-11\n",
            "Step: 11800 | Loss: 5.6831e-14 | Loss_d: 0.0000e+00 | Loss_e: 5.6831e-13 | lamb: 1.080000 | Dp: 5.5556e-11\n",
            "Step: 11850 | Loss: 5.4385e-14 | Loss_d: 0.0000e+00 | Loss_e: 5.4385e-13 | lamb: 1.080000 | Dp: 5.0265e-11\n",
            "Step: 11900 | Loss: 5.2128e-14 | Loss_d: 0.0000e+00 | Loss_e: 5.2128e-13 | lamb: 1.080000 | Dp: 4.5477e-11\n",
            "Step: 11950 | Loss: 5.0035e-14 | Loss_d: 0.0000e+00 | Loss_e: 5.0035e-13 | lamb: 1.080000 | Dp: 4.1145e-11\n",
            "Step: 12000 | Loss: 4.8086e-14 | Loss_d: 0.0000e+00 | Loss_e: 4.8086e-13 | lamb: 1.080000 | Dp: 3.7226e-11\n",
            "Step: 12050 | Loss: 9.0415e-14 | Loss_d: 0.0000e+00 | Loss_e: 9.0415e-13 | lamb: 1.080000 | Dp: 3.3680e-11\n",
            "Step: 12100 | Loss: 8.4688e-14 | Loss_d: 0.0000e+00 | Loss_e: 8.4688e-13 | lamb: 1.080000 | Dp: 3.0472e-11\n",
            "Step: 12150 | Loss: 7.9899e-14 | Loss_d: 0.0000e+00 | Loss_e: 7.9899e-13 | lamb: 1.080000 | Dp: 2.7569e-11\n",
            "Step: 12200 | Loss: 7.4402e-14 | Loss_d: 0.0000e+00 | Loss_e: 7.4402e-13 | lamb: 1.080000 | Dp: 2.4943e-11\n",
            "Step: 12250 | Loss: 6.8553e-14 | Loss_d: 0.0000e+00 | Loss_e: 6.8553e-13 | lamb: 1.080000 | Dp: 2.2567e-11\n",
            "Step: 12300 | Loss: 6.3367e-14 | Loss_d: 0.0000e+00 | Loss_e: 6.3367e-13 | lamb: 1.080000 | Dp: 2.0418e-11\n",
            "Step: 12350 | Loss: 5.8533e-14 | Loss_d: 0.0000e+00 | Loss_e: 5.8533e-13 | lamb: 1.080000 | Dp: 1.8473e-11\n",
            "Step: 12400 | Loss: 5.3800e-14 | Loss_d: 0.0000e+00 | Loss_e: 5.3800e-13 | lamb: 1.080000 | Dp: 1.6713e-11\n",
            "Step: 12450 | Loss: 4.9582e-14 | Loss_d: 0.0000e+00 | Loss_e: 4.9582e-13 | lamb: 1.080000 | Dp: 1.5121e-11\n",
            "Step: 12500 | Loss: 4.5747e-14 | Loss_d: 0.0000e+00 | Loss_e: 4.5747e-13 | lamb: 1.080000 | Dp: 1.3681e-11\n",
            "Step: 12550 | Loss: 4.2263e-14 | Loss_d: 0.0000e+00 | Loss_e: 4.2263e-13 | lamb: 1.080000 | Dp: 1.2378e-11\n",
            "Step: 12600 | Loss: 3.8984e-14 | Loss_d: 0.0000e+00 | Loss_e: 3.8984e-13 | lamb: 1.080000 | Dp: 1.1199e-11\n",
            "Step: 12650 | Loss: 3.6008e-14 | Loss_d: 0.0000e+00 | Loss_e: 3.6008e-13 | lamb: 1.080000 | Dp: 1.0132e-11\n",
            "Step: 12700 | Loss: 3.3181e-14 | Loss_d: 0.0000e+00 | Loss_e: 3.3181e-13 | lamb: 1.080000 | Dp: 9.1669e-12\n",
            "Step: 12750 | Loss: 3.0510e-14 | Loss_d: 0.0000e+00 | Loss_e: 3.0510e-13 | lamb: 1.080000 | Dp: 8.2937e-12\n",
            "Step: 12800 | Loss: 2.8011e-14 | Loss_d: 0.0000e+00 | Loss_e: 2.8011e-13 | lamb: 1.080000 | Dp: 7.5037e-12\n",
            "Step: 12850 | Loss: 2.5735e-14 | Loss_d: 0.0000e+00 | Loss_e: 2.5735e-13 | lamb: 1.080000 | Dp: 6.7890e-12\n",
            "Step: 12900 | Loss: 2.3721e-14 | Loss_d: 0.0000e+00 | Loss_e: 2.3721e-13 | lamb: 1.080000 | Dp: 6.1423e-12\n",
            "Step: 12950 | Loss: 2.1979e-14 | Loss_d: 0.0000e+00 | Loss_e: 2.1979e-13 | lamb: 1.080000 | Dp: 5.5572e-12\n",
            "Step: 13000 | Loss: 2.0480e-14 | Loss_d: 0.0000e+00 | Loss_e: 2.0480e-13 | lamb: 1.080000 | Dp: 5.0279e-12\n",
            "Step: 13050 | Loss: 1.8170e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.8170e-13 | lamb: 1.080000 | Dp: 4.5489e-12\n",
            "Step: 13100 | Loss: 1.7158e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.7158e-13 | lamb: 1.080000 | Dp: 4.1156e-12\n",
            "Step: 13150 | Loss: 1.6492e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.6492e-13 | lamb: 1.080000 | Dp: 3.7236e-12\n",
            "Step: 13200 | Loss: 1.5853e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.5853e-13 | lamb: 1.080000 | Dp: 3.3689e-12\n",
            "Step: 13250 | Loss: 1.5141e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.5141e-13 | lamb: 1.080000 | Dp: 3.0480e-12\n",
            "Step: 13300 | Loss: 1.4411e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.4411e-13 | lamb: 1.080000 | Dp: 2.7577e-12\n",
            "Step: 13350 | Loss: 1.3701e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.3701e-13 | lamb: 1.080000 | Dp: 2.4950e-12\n",
            "Step: 13400 | Loss: 1.3035e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.3035e-13 | lamb: 1.080000 | Dp: 2.2574e-12\n",
            "Step: 13450 | Loss: 1.2416e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.2416e-13 | lamb: 1.080000 | Dp: 2.0423e-12\n",
            "Step: 13500 | Loss: 1.1832e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.1832e-13 | lamb: 1.080000 | Dp: 1.8478e-12\n",
            "Step: 13550 | Loss: 1.1269e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.1269e-13 | lamb: 1.080000 | Dp: 1.6718e-12\n",
            "Step: 13600 | Loss: 1.0723e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.0723e-13 | lamb: 1.080000 | Dp: 1.5125e-12\n",
            "Step: 13650 | Loss: 1.0197e-14 | Loss_d: 0.0000e+00 | Loss_e: 1.0197e-13 | lamb: 1.080000 | Dp: 1.3685e-12\n",
            "Step: 13700 | Loss: 9.6804e-15 | Loss_d: 0.0000e+00 | Loss_e: 9.6804e-14 | lamb: 1.080000 | Dp: 1.2381e-12\n",
            "Step: 13750 | Loss: 9.1686e-15 | Loss_d: 0.0000e+00 | Loss_e: 9.1686e-14 | lamb: 1.080000 | Dp: 1.1202e-12\n",
            "Step: 13800 | Loss: 8.6684e-15 | Loss_d: 0.0000e+00 | Loss_e: 8.6684e-14 | lamb: 1.080000 | Dp: 1.0135e-12\n",
            "Step: 13850 | Loss: 8.1928e-15 | Loss_d: 0.0000e+00 | Loss_e: 8.1928e-14 | lamb: 1.080000 | Dp: 9.1695e-13\n",
            "Step: 13900 | Loss: 7.7603e-15 | Loss_d: 0.0000e+00 | Loss_e: 7.7603e-14 | lamb: 1.080000 | Dp: 8.2960e-13\n",
            "Step: 13950 | Loss: 7.3709e-15 | Loss_d: 0.0000e+00 | Loss_e: 7.3709e-14 | lamb: 1.080000 | Dp: 7.5058e-13\n",
            "Step: 14000 | Loss: 7.0098e-15 | Loss_d: 0.0000e+00 | Loss_e: 7.0098e-14 | lamb: 1.080000 | Dp: 6.7909e-13\n",
            "Step: 14050 | Loss: 5.8730e-15 | Loss_d: 0.0000e+00 | Loss_e: 5.8730e-14 | lamb: 1.080000 | Dp: 6.1440e-13\n",
            "Step: 14100 | Loss: 4.9111e-15 | Loss_d: 0.0000e+00 | Loss_e: 4.9111e-14 | lamb: 1.080000 | Dp: 5.5588e-13\n",
            "Step: 14150 | Loss: 4.2118e-15 | Loss_d: 0.0000e+00 | Loss_e: 4.2118e-14 | lamb: 1.080000 | Dp: 5.0293e-13\n",
            "Step: 14200 | Loss: 3.5488e-15 | Loss_d: 0.0000e+00 | Loss_e: 3.5488e-14 | lamb: 1.080000 | Dp: 4.5502e-13\n",
            "Step: 14250 | Loss: 3.0264e-15 | Loss_d: 0.0000e+00 | Loss_e: 3.0264e-14 | lamb: 1.080000 | Dp: 4.1168e-13\n",
            "Step: 14300 | Loss: 2.6186e-15 | Loss_d: 0.0000e+00 | Loss_e: 2.6186e-14 | lamb: 1.080000 | Dp: 3.7247e-13\n",
            "Step: 14350 | Loss: 2.3397e-15 | Loss_d: 0.0000e+00 | Loss_e: 2.3397e-14 | lamb: 1.080000 | Dp: 3.3699e-13\n",
            "Step: 14400 | Loss: 2.1472e-15 | Loss_d: 0.0000e+00 | Loss_e: 2.1472e-14 | lamb: 1.080000 | Dp: 3.0489e-13\n",
            "Step: 14450 | Loss: 2.0148e-15 | Loss_d: 0.0000e+00 | Loss_e: 2.0148e-14 | lamb: 1.080000 | Dp: 2.7585e-13\n",
            "Step: 14500 | Loss: 1.9150e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.9150e-14 | lamb: 1.080000 | Dp: 2.4957e-13\n",
            "Step: 14550 | Loss: 1.8338e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.8338e-14 | lamb: 1.080000 | Dp: 2.2580e-13\n",
            "Step: 14600 | Loss: 1.7655e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.7655e-14 | lamb: 1.080000 | Dp: 2.0429e-13\n",
            "Step: 14650 | Loss: 1.7055e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.7055e-14 | lamb: 1.080000 | Dp: 1.8483e-13\n",
            "Step: 14700 | Loss: 1.6515e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.6515e-14 | lamb: 1.080000 | Dp: 1.6723e-13\n",
            "Step: 14750 | Loss: 1.6019e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.6019e-14 | lamb: 1.080000 | Dp: 1.5130e-13\n",
            "Step: 14800 | Loss: 1.5554e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.5554e-14 | lamb: 1.080000 | Dp: 1.3689e-13\n",
            "Step: 14850 | Loss: 1.5108e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.5108e-14 | lamb: 1.080000 | Dp: 1.2385e-13\n",
            "Step: 14900 | Loss: 1.4670e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.4670e-14 | lamb: 1.080000 | Dp: 1.1205e-13\n",
            "Step: 14950 | Loss: 1.4232e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.4232e-14 | lamb: 1.080000 | Dp: 1.0138e-13\n",
            "Step: 15000 | Loss: 1.3790e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.3790e-14 | lamb: 1.080000 | Dp: 9.1721e-14\n",
            "Step: 15050 | Loss: 4.6101e-15 | Loss_d: 0.0000e+00 | Loss_e: 4.6101e-14 | lamb: 1.080000 | Dp: 8.2984e-14\n",
            "Step: 15100 | Loss: 3.9951e-15 | Loss_d: 0.0000e+00 | Loss_e: 3.9951e-14 | lamb: 1.080000 | Dp: 7.5079e-14\n",
            "Step: 15150 | Loss: 3.6407e-15 | Loss_d: 0.0000e+00 | Loss_e: 3.6407e-14 | lamb: 1.080000 | Dp: 6.7928e-14\n",
            "Step: 15200 | Loss: 3.3124e-15 | Loss_d: 0.0000e+00 | Loss_e: 3.3124e-14 | lamb: 1.080000 | Dp: 6.1457e-14\n",
            "Step: 15250 | Loss: 2.9744e-15 | Loss_d: 0.0000e+00 | Loss_e: 2.9744e-14 | lamb: 1.080000 | Dp: 5.5603e-14\n",
            "Step: 15300 | Loss: 2.6777e-15 | Loss_d: 0.0000e+00 | Loss_e: 2.6777e-14 | lamb: 1.080000 | Dp: 5.0307e-14\n",
            "Step: 15350 | Loss: 2.4690e-15 | Loss_d: 0.0000e+00 | Loss_e: 2.4690e-14 | lamb: 1.080000 | Dp: 4.5515e-14\n",
            "Step: 15400 | Loss: 2.2575e-15 | Loss_d: 0.0000e+00 | Loss_e: 2.2575e-14 | lamb: 1.080000 | Dp: 4.1180e-14\n",
            "Step: 15450 | Loss: 2.0605e-15 | Loss_d: 0.0000e+00 | Loss_e: 2.0605e-14 | lamb: 1.080000 | Dp: 3.7257e-14\n",
            "Step: 15500 | Loss: 1.9038e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.9038e-14 | lamb: 1.080000 | Dp: 3.3708e-14\n",
            "Step: 15550 | Loss: 1.7429e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.7429e-14 | lamb: 1.080000 | Dp: 3.0498e-14\n",
            "Step: 15600 | Loss: 1.5878e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.5878e-14 | lamb: 1.080000 | Dp: 2.7593e-14\n",
            "Step: 15650 | Loss: 1.4890e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.4890e-14 | lamb: 1.080000 | Dp: 2.4964e-14\n",
            "Step: 15700 | Loss: 1.3720e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.3720e-14 | lamb: 1.080000 | Dp: 2.2586e-14\n",
            "Step: 15750 | Loss: 1.2854e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.2854e-14 | lamb: 1.080000 | Dp: 2.0435e-14\n",
            "Step: 15800 | Loss: 1.2000e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.2000e-14 | lamb: 1.080000 | Dp: 1.8488e-14\n",
            "Step: 15850 | Loss: 1.1058e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.1058e-14 | lamb: 1.080000 | Dp: 1.6727e-14\n",
            "Step: 15900 | Loss: 1.0176e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.0176e-14 | lamb: 1.080000 | Dp: 1.5134e-14\n",
            "Step: 15950 | Loss: 9.4736e-16 | Loss_d: 0.0000e+00 | Loss_e: 9.4736e-15 | lamb: 1.080000 | Dp: 1.3692e-14\n",
            "Step: 16000 | Loss: 8.8415e-16 | Loss_d: 0.0000e+00 | Loss_e: 8.8415e-15 | lamb: 1.080000 | Dp: 1.2388e-14\n",
            "Step: 16050 | Loss: 1.1466e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.1466e-14 | lamb: 1.080000 | Dp: 1.1208e-14\n",
            "Step: 16100 | Loss: 8.8490e-16 | Loss_d: 0.0000e+00 | Loss_e: 8.8490e-15 | lamb: 1.080000 | Dp: 1.0141e-14\n",
            "Step: 16150 | Loss: 6.4934e-16 | Loss_d: 0.0000e+00 | Loss_e: 6.4934e-15 | lamb: 1.080000 | Dp: 9.1746e-15\n",
            "Step: 16200 | Loss: 5.6220e-16 | Loss_d: 0.0000e+00 | Loss_e: 5.6220e-15 | lamb: 1.080000 | Dp: 8.3007e-15\n",
            "Step: 16250 | Loss: 5.0840e-16 | Loss_d: 0.0000e+00 | Loss_e: 5.0840e-15 | lamb: 1.080000 | Dp: 7.5101e-15\n",
            "Step: 16300 | Loss: 4.8481e-16 | Loss_d: 0.0000e+00 | Loss_e: 4.8481e-15 | lamb: 1.080000 | Dp: 6.7947e-15\n",
            "Step: 16350 | Loss: 4.5871e-16 | Loss_d: 0.0000e+00 | Loss_e: 4.5871e-15 | lamb: 1.080000 | Dp: 6.1475e-15\n",
            "Step: 16400 | Loss: 4.3728e-16 | Loss_d: 0.0000e+00 | Loss_e: 4.3728e-15 | lamb: 1.080000 | Dp: 5.5619e-15\n",
            "Step: 16450 | Loss: 4.2067e-16 | Loss_d: 0.0000e+00 | Loss_e: 4.2067e-15 | lamb: 1.080000 | Dp: 5.0321e-15\n",
            "Step: 16500 | Loss: 4.0337e-16 | Loss_d: 0.0000e+00 | Loss_e: 4.0337e-15 | lamb: 1.080000 | Dp: 4.5528e-15\n",
            "Step: 16550 | Loss: 3.9291e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.9291e-15 | lamb: 1.080000 | Dp: 4.1191e-15\n",
            "Step: 16600 | Loss: 3.7571e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.7571e-15 | lamb: 1.080000 | Dp: 3.7268e-15\n",
            "Step: 16650 | Loss: 3.5846e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.5846e-15 | lamb: 1.080000 | Dp: 3.3718e-15\n",
            "Step: 16700 | Loss: 3.4338e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.4338e-15 | lamb: 1.080000 | Dp: 3.0506e-15\n",
            "Step: 16750 | Loss: 3.2896e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.2896e-15 | lamb: 1.080000 | Dp: 2.7600e-15\n",
            "Step: 16800 | Loss: 3.1557e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.1557e-15 | lamb: 1.080000 | Dp: 2.4971e-15\n",
            "Step: 16850 | Loss: 3.0396e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.0396e-15 | lamb: 1.080000 | Dp: 2.2593e-15\n",
            "Step: 16900 | Loss: 2.9063e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.9063e-15 | lamb: 1.080000 | Dp: 2.0441e-15\n",
            "Step: 16950 | Loss: 2.7618e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.7618e-15 | lamb: 1.080000 | Dp: 1.8494e-15\n",
            "Step: 17000 | Loss: 2.6609e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.6609e-15 | lamb: 1.080000 | Dp: 1.6732e-15\n",
            "Step: 17050 | Loss: 1.5487e-15 | Loss_d: 0.0000e+00 | Loss_e: 1.5487e-14 | lamb: 1.080000 | Dp: 1.5138e-15\n",
            "Step: 17100 | Loss: 9.0665e-16 | Loss_d: 0.0000e+00 | Loss_e: 9.0665e-15 | lamb: 1.080000 | Dp: 1.3696e-15\n",
            "Step: 17150 | Loss: 6.9882e-16 | Loss_d: 0.0000e+00 | Loss_e: 6.9882e-15 | lamb: 1.080000 | Dp: 1.2392e-15\n",
            "Step: 17200 | Loss: 6.2875e-16 | Loss_d: 0.0000e+00 | Loss_e: 6.2875e-15 | lamb: 1.080000 | Dp: 1.1211e-15\n",
            "Step: 17250 | Loss: 5.9043e-16 | Loss_d: 0.0000e+00 | Loss_e: 5.9043e-15 | lamb: 1.080000 | Dp: 1.0143e-15\n",
            "Step: 17300 | Loss: 5.4455e-16 | Loss_d: 0.0000e+00 | Loss_e: 5.4455e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17350 | Loss: 5.1279e-16 | Loss_d: 0.0000e+00 | Loss_e: 5.1279e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17400 | Loss: 4.8476e-16 | Loss_d: 0.0000e+00 | Loss_e: 4.8476e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17450 | Loss: 4.5626e-16 | Loss_d: 0.0000e+00 | Loss_e: 4.5626e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17500 | Loss: 4.3338e-16 | Loss_d: 0.0000e+00 | Loss_e: 4.3338e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17550 | Loss: 4.0734e-16 | Loss_d: 0.0000e+00 | Loss_e: 4.0734e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17600 | Loss: 3.8644e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.8644e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17650 | Loss: 3.6239e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.6239e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17700 | Loss: 3.4177e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.4177e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17750 | Loss: 3.2370e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.2370e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17800 | Loss: 3.1347e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.1347e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17850 | Loss: 2.9759e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.9759e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17900 | Loss: 2.8634e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.8634e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 17950 | Loss: 2.7412e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.7412e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18000 | Loss: 2.6645e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.6645e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18050 | Loss: 2.0960e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.0960e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18100 | Loss: 1.7912e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.7912e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18150 | Loss: 1.7021e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.7021e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18200 | Loss: 1.6176e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.6176e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18250 | Loss: 1.5284e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.5284e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18300 | Loss: 1.4424e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.4424e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18350 | Loss: 1.3813e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.3813e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18400 | Loss: 1.3081e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.3081e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18450 | Loss: 1.2325e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.2325e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18500 | Loss: 1.1914e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.1914e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18550 | Loss: 1.1412e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.1412e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18600 | Loss: 1.1111e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.1111e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18650 | Loss: 1.0600e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.0600e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18700 | Loss: 1.0375e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.0375e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18750 | Loss: 1.0078e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.0078e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18800 | Loss: 9.8742e-17 | Loss_d: 0.0000e+00 | Loss_e: 9.8742e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18850 | Loss: 9.6433e-17 | Loss_d: 0.0000e+00 | Loss_e: 9.6433e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18900 | Loss: 9.3934e-17 | Loss_d: 0.0000e+00 | Loss_e: 9.3934e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 18950 | Loss: 9.2056e-17 | Loss_d: 0.0000e+00 | Loss_e: 9.2056e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19000 | Loss: 9.0522e-17 | Loss_d: 0.0000e+00 | Loss_e: 9.0522e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19050 | Loss: 3.1472e-16 | Loss_d: 0.0000e+00 | Loss_e: 3.1472e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19100 | Loss: 2.8061e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.8061e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19150 | Loss: 2.6906e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.6906e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19200 | Loss: 2.6029e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.6029e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19250 | Loss: 2.5032e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.5032e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19300 | Loss: 2.3942e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.3942e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19350 | Loss: 2.3293e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.3293e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19400 | Loss: 2.2581e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.2581e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19450 | Loss: 2.1795e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.1795e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19500 | Loss: 2.1298e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.1298e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19550 | Loss: 2.0736e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.0736e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19600 | Loss: 2.0018e-16 | Loss_d: 0.0000e+00 | Loss_e: 2.0018e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19650 | Loss: 1.9171e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.9171e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19700 | Loss: 1.8715e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.8715e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19750 | Loss: 1.8054e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.8054e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19800 | Loss: 1.7423e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.7423e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19850 | Loss: 1.6845e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.6845e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19900 | Loss: 1.6473e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.6473e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 19950 | Loss: 1.5955e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.5955e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20000 | Loss: 1.5639e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.5639e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20050 | Loss: 1.8982e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.8982e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20100 | Loss: 1.8476e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.8476e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20150 | Loss: 1.7991e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.7991e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20200 | Loss: 1.7377e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.7377e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20250 | Loss: 1.6946e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.6946e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20300 | Loss: 1.6590e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.6590e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20350 | Loss: 1.6052e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.6052e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20400 | Loss: 1.5425e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.5425e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20450 | Loss: 1.4854e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.4854e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20500 | Loss: 1.4160e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.4160e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20550 | Loss: 1.3593e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.3593e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20600 | Loss: 1.3117e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.3117e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20650 | Loss: 1.2676e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.2676e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20700 | Loss: 1.2134e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.2134e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20750 | Loss: 1.1655e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.1655e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20800 | Loss: 1.1331e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.1331e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20850 | Loss: 1.0815e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.0815e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20900 | Loss: 1.0511e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.0511e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 20950 | Loss: 1.0094e-16 | Loss_d: 0.0000e+00 | Loss_e: 1.0094e-15 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21000 | Loss: 9.7433e-17 | Loss_d: 0.0000e+00 | Loss_e: 9.7433e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21050 | Loss: 5.1235e-17 | Loss_d: 0.0000e+00 | Loss_e: 5.1235e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21100 | Loss: 4.4147e-17 | Loss_d: 0.0000e+00 | Loss_e: 4.4147e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21150 | Loss: 4.0982e-17 | Loss_d: 0.0000e+00 | Loss_e: 4.0982e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21200 | Loss: 3.9362e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.9362e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21250 | Loss: 3.7480e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.7480e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21300 | Loss: 3.5685e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.5685e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21350 | Loss: 3.4542e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.4542e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21400 | Loss: 3.3013e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.3013e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21450 | Loss: 3.1963e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.1963e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21500 | Loss: 3.1161e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.1161e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21550 | Loss: 3.0259e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.0259e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21600 | Loss: 2.9137e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.9137e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21650 | Loss: 2.8224e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.8224e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21700 | Loss: 2.7444e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.7444e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21750 | Loss: 2.6729e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.6729e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21800 | Loss: 2.5906e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.5906e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21850 | Loss: 2.5399e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.5399e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21900 | Loss: 2.4861e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.4861e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 21950 | Loss: 2.4279e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.4279e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22000 | Loss: 2.3775e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.3775e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22050 | Loss: 3.4862e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.4862e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22100 | Loss: 2.2119e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.2119e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22150 | Loss: 1.7450e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7450e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22200 | Loss: 1.5770e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.5770e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22250 | Loss: 1.4515e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.4515e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22300 | Loss: 1.3702e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3702e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22350 | Loss: 1.3014e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3014e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22400 | Loss: 1.2348e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.2348e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22450 | Loss: 1.1968e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1968e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22500 | Loss: 1.1628e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1628e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22550 | Loss: 1.1333e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1333e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22600 | Loss: 1.1032e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1032e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22650 | Loss: 1.0813e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0813e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22700 | Loss: 1.0690e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0690e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22750 | Loss: 1.0536e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0536e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22800 | Loss: 1.0389e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0389e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22850 | Loss: 1.0254e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0254e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22900 | Loss: 1.0097e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0097e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 22950 | Loss: 1.0011e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0011e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23000 | Loss: 9.9319e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.9319e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23050 | Loss: 2.5698e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.5698e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23100 | Loss: 1.8176e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8176e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23150 | Loss: 1.4468e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.4468e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23200 | Loss: 1.3179e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3179e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23250 | Loss: 1.2535e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.2535e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23300 | Loss: 1.2057e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.2057e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23350 | Loss: 1.1748e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1748e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23400 | Loss: 1.1424e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1424e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23450 | Loss: 1.1160e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1160e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23500 | Loss: 1.0946e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0946e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23550 | Loss: 1.0768e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0768e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23600 | Loss: 1.0662e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0662e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23650 | Loss: 1.0520e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0520e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23700 | Loss: 1.0401e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0401e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23750 | Loss: 1.0336e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0336e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23800 | Loss: 1.0235e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0235e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23850 | Loss: 1.0112e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0112e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23900 | Loss: 1.0052e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0052e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 23950 | Loss: 9.9750e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.9750e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24000 | Loss: 9.8938e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.8938e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24050 | Loss: 2.9919e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.9919e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24100 | Loss: 2.6102e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.6102e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24150 | Loss: 2.5556e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.5556e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24200 | Loss: 2.4825e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.4825e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24250 | Loss: 2.3810e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.3810e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24300 | Loss: 2.3294e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.3294e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24350 | Loss: 2.2717e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.2717e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24400 | Loss: 2.2272e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.2272e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24450 | Loss: 2.1794e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.1794e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24500 | Loss: 2.1252e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.1252e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24550 | Loss: 2.0775e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.0775e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24600 | Loss: 2.0238e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.0238e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24650 | Loss: 1.9757e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.9757e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24700 | Loss: 1.9405e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.9405e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24750 | Loss: 1.9052e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.9052e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24800 | Loss: 1.8702e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8702e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24850 | Loss: 1.8330e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8330e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24900 | Loss: 1.8068e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8068e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 24950 | Loss: 1.7629e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7629e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25000 | Loss: 1.7277e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7277e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25050 | Loss: 1.2028e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.2028e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25100 | Loss: 1.0260e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0260e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25150 | Loss: 9.2769e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.2769e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25200 | Loss: 8.8945e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.8945e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25250 | Loss: 8.6756e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.6756e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25300 | Loss: 8.5457e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.5457e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25350 | Loss: 8.4744e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.4744e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25400 | Loss: 8.4132e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.4132e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25450 | Loss: 8.3608e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.3608e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25500 | Loss: 8.3136e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.3136e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25550 | Loss: 8.2694e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.2694e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25600 | Loss: 8.2415e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.2415e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25650 | Loss: 8.2018e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.2018e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25700 | Loss: 8.1643e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.1643e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25750 | Loss: 8.1343e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.1343e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25800 | Loss: 8.1041e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.1041e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25850 | Loss: 8.0711e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.0711e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25900 | Loss: 8.0397e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.0397e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 25950 | Loss: 8.0001e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.0001e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26000 | Loss: 7.9699e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.9699e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26050 | Loss: 3.9178e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.9178e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26100 | Loss: 3.3773e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.3773e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26150 | Loss: 3.1898e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.1898e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26200 | Loss: 3.0264e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.0264e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26250 | Loss: 2.8949e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.8949e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26300 | Loss: 2.8072e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.8072e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26350 | Loss: 2.7080e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.7080e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26400 | Loss: 2.6181e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.6181e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26450 | Loss: 2.5566e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.5566e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26500 | Loss: 2.5045e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.5045e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26550 | Loss: 2.4546e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.4546e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26600 | Loss: 2.4087e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.4087e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26650 | Loss: 2.3752e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.3752e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26700 | Loss: 2.3383e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.3383e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26750 | Loss: 2.3094e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.3094e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26800 | Loss: 2.2745e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.2745e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26850 | Loss: 2.2536e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.2536e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26900 | Loss: 2.2286e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.2286e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 26950 | Loss: 2.1909e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.1909e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27000 | Loss: 2.1668e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.1668e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27050 | Loss: 1.4280e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.4280e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27100 | Loss: 1.3480e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3480e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27150 | Loss: 1.3200e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3200e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27200 | Loss: 1.2941e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.2941e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27250 | Loss: 1.2654e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.2654e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27300 | Loss: 1.2354e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.2354e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27350 | Loss: 1.2044e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.2044e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27400 | Loss: 1.1812e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1812e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27450 | Loss: 1.1639e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1639e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27500 | Loss: 1.1387e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1387e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27550 | Loss: 1.1253e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1253e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27600 | Loss: 1.1116e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1116e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27650 | Loss: 1.0964e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0964e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27700 | Loss: 1.0836e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0836e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27750 | Loss: 1.0708e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0708e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27800 | Loss: 1.0618e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0618e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27850 | Loss: 1.0491e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0491e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27900 | Loss: 1.0341e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0341e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 27950 | Loss: 1.0217e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0217e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28000 | Loss: 1.0123e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0123e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28050 | Loss: 2.0147e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.0147e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28100 | Loss: 1.6812e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.6812e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28150 | Loss: 1.6294e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.6294e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28200 | Loss: 1.5943e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.5943e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28250 | Loss: 1.5664e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.5664e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28300 | Loss: 1.5398e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.5398e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28350 | Loss: 1.5166e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.5166e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28400 | Loss: 1.4931e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.4931e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28450 | Loss: 1.4748e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.4748e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28500 | Loss: 1.4566e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.4566e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28550 | Loss: 1.4383e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.4383e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28600 | Loss: 1.4264e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.4264e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28650 | Loss: 1.4124e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.4124e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28700 | Loss: 1.3963e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3963e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28750 | Loss: 1.3847e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3847e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28800 | Loss: 1.3717e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3717e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28850 | Loss: 1.3561e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3561e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28900 | Loss: 1.3424e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3424e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 28950 | Loss: 1.3296e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3296e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29000 | Loss: 1.3219e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.3219e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29050 | Loss: 1.9619e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.9619e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29100 | Loss: 1.9061e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.9061e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29150 | Loss: 1.8839e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8839e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29200 | Loss: 1.8634e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8634e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29250 | Loss: 1.8508e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8508e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29300 | Loss: 1.8324e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8324e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29350 | Loss: 1.8204e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8204e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29400 | Loss: 1.8086e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.8086e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29450 | Loss: 1.7959e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7959e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29500 | Loss: 1.7843e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7843e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29550 | Loss: 1.7728e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7728e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29600 | Loss: 1.7596e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7596e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29650 | Loss: 1.7447e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7447e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29700 | Loss: 1.7308e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7308e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29750 | Loss: 1.7122e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7122e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29800 | Loss: 1.7002e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.7002e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29850 | Loss: 1.6848e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.6848e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29900 | Loss: 1.6733e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.6733e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 29950 | Loss: 1.6613e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.6613e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30000 | Loss: 1.6467e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.6467e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30050 | Loss: 3.5770e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.5770e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30100 | Loss: 3.1907e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.1907e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30150 | Loss: 3.0316e-17 | Loss_d: 0.0000e+00 | Loss_e: 3.0316e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30200 | Loss: 2.9326e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.9326e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30250 | Loss: 2.8734e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.8734e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30300 | Loss: 2.8203e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.8203e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30350 | Loss: 2.7686e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.7686e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30400 | Loss: 2.7254e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.7254e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30450 | Loss: 2.6720e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.6720e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30500 | Loss: 2.6311e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.6311e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30550 | Loss: 2.5883e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.5883e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30600 | Loss: 2.5476e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.5476e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30650 | Loss: 2.4976e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.4976e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30700 | Loss: 2.4586e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.4586e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30750 | Loss: 2.4228e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.4228e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30800 | Loss: 2.3754e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.3754e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30850 | Loss: 2.3286e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.3286e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30900 | Loss: 2.2875e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.2875e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 30950 | Loss: 2.2468e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.2468e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31000 | Loss: 2.2049e-17 | Loss_d: 0.0000e+00 | Loss_e: 2.2049e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31050 | Loss: 1.0929e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0929e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31100 | Loss: 9.8286e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.8286e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31150 | Loss: 9.3222e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.3222e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31200 | Loss: 8.9004e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.9004e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31250 | Loss: 8.5268e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.5268e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31300 | Loss: 8.2822e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.2822e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31350 | Loss: 8.0975e-18 | Loss_d: 0.0000e+00 | Loss_e: 8.0975e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31400 | Loss: 7.8767e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.8767e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31450 | Loss: 7.7311e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.7311e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31500 | Loss: 7.5977e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.5977e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31550 | Loss: 7.4307e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.4307e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31600 | Loss: 7.3148e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.3148e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31650 | Loss: 7.2091e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.2091e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31700 | Loss: 7.1231e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.1231e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31750 | Loss: 7.0303e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.0303e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31800 | Loss: 6.9401e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.9401e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31850 | Loss: 6.8412e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.8412e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31900 | Loss: 6.7277e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.7277e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 31950 | Loss: 6.6544e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.6544e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32000 | Loss: 6.5814e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.5814e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32050 | Loss: 9.3749e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.3749e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32100 | Loss: 7.8075e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.8075e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32150 | Loss: 7.2769e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.2769e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32200 | Loss: 6.9934e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.9934e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32250 | Loss: 6.7671e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.7671e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32300 | Loss: 6.5746e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.5746e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32350 | Loss: 6.4732e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.4732e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32400 | Loss: 6.3284e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.3284e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32450 | Loss: 6.2330e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.2330e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32500 | Loss: 6.1687e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.1687e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32550 | Loss: 6.0984e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.0984e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32600 | Loss: 6.0554e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.0554e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32650 | Loss: 6.0072e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.0072e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32700 | Loss: 5.9456e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.9456e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32750 | Loss: 5.9037e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.9037e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32800 | Loss: 5.8758e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.8758e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32850 | Loss: 5.8311e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.8311e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32900 | Loss: 5.7959e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.7959e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 32950 | Loss: 5.7534e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.7534e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33000 | Loss: 5.7112e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.7112e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33050 | Loss: 1.1619e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1619e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33100 | Loss: 1.1286e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1286e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33150 | Loss: 1.1180e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1180e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33200 | Loss: 1.1050e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.1050e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33250 | Loss: 1.0957e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0957e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33300 | Loss: 1.0844e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0844e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33350 | Loss: 1.0742e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0742e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33400 | Loss: 1.0633e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0633e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33450 | Loss: 1.0508e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0508e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33500 | Loss: 1.0371e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0371e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33550 | Loss: 1.0269e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0269e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33600 | Loss: 1.0163e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0163e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33650 | Loss: 1.0059e-17 | Loss_d: 0.0000e+00 | Loss_e: 1.0059e-16 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33700 | Loss: 9.9908e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.9908e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33750 | Loss: 9.8553e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.8553e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33800 | Loss: 9.7721e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.7721e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33850 | Loss: 9.6916e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.6916e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33900 | Loss: 9.6023e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.6023e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 33950 | Loss: 9.5194e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.5194e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34000 | Loss: 9.4209e-18 | Loss_d: 0.0000e+00 | Loss_e: 9.4209e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34050 | Loss: 5.3214e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.3214e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34100 | Loss: 4.1617e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.1617e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34150 | Loss: 3.7144e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.7144e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34200 | Loss: 3.4638e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.4638e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34250 | Loss: 3.2856e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.2856e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34300 | Loss: 3.1596e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.1596e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34350 | Loss: 3.0400e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.0400e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34400 | Loss: 2.8611e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.8611e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34450 | Loss: 2.7078e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.7078e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34500 | Loss: 2.6378e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.6378e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34550 | Loss: 2.4841e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.4841e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34600 | Loss: 2.4216e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.4216e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34650 | Loss: 2.3343e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.3343e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34700 | Loss: 2.2735e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.2735e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34750 | Loss: 2.2300e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.2300e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34800 | Loss: 2.1581e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.1581e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34850 | Loss: 2.1077e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.1077e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34900 | Loss: 2.0564e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.0564e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 34950 | Loss: 2.0163e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.0163e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35000 | Loss: 1.9625e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.9625e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35050 | Loss: 5.3929e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.3929e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35100 | Loss: 4.8252e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.8252e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35150 | Loss: 4.6841e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.6841e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35200 | Loss: 4.5632e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.5632e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35250 | Loss: 4.4732e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.4732e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35300 | Loss: 4.4052e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.4052e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35350 | Loss: 4.3472e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.3472e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35400 | Loss: 4.2859e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.2859e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35450 | Loss: 4.2041e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.2041e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35500 | Loss: 4.1550e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.1550e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35550 | Loss: 4.1057e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.1057e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35600 | Loss: 4.0356e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.0356e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35650 | Loss: 3.9926e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.9926e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35700 | Loss: 3.9533e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.9533e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35750 | Loss: 3.9015e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.9015e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35800 | Loss: 3.8562e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.8562e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35850 | Loss: 3.8109e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.8109e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35900 | Loss: 3.7797e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.7797e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 35950 | Loss: 3.7420e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.7420e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36000 | Loss: 3.6988e-18 | Loss_d: 0.0000e+00 | Loss_e: 3.6988e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36050 | Loss: 7.6327e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.6327e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36100 | Loss: 7.2536e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.2536e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36150 | Loss: 7.1848e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.1848e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36200 | Loss: 7.0903e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.0903e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36250 | Loss: 7.0103e-18 | Loss_d: 0.0000e+00 | Loss_e: 7.0103e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36300 | Loss: 6.9411e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.9411e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36350 | Loss: 6.8442e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.8442e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36400 | Loss: 6.7325e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.7325e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36450 | Loss: 6.6432e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.6432e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36500 | Loss: 6.5779e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.5779e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36550 | Loss: 6.4866e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.4866e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36600 | Loss: 6.3946e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.3946e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36650 | Loss: 6.3010e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.3010e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36700 | Loss: 6.2162e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.2162e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36750 | Loss: 6.0916e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.0916e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36800 | Loss: 6.0322e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.0322e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36850 | Loss: 5.9097e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.9097e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36900 | Loss: 5.7967e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.7967e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 36950 | Loss: 5.7060e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.7060e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37000 | Loss: 5.6247e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.6247e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37050 | Loss: 6.3013e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.3013e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37100 | Loss: 6.0747e-18 | Loss_d: 0.0000e+00 | Loss_e: 6.0747e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37150 | Loss: 5.9287e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.9287e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37200 | Loss: 5.8046e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.8046e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37250 | Loss: 5.7412e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.7412e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37300 | Loss: 5.6317e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.6317e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37350 | Loss: 5.5337e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.5337e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37400 | Loss: 5.4141e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.4141e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37450 | Loss: 5.3289e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.3289e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37500 | Loss: 5.2133e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.2133e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37550 | Loss: 5.1199e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.1199e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37600 | Loss: 5.0465e-18 | Loss_d: 0.0000e+00 | Loss_e: 5.0465e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37650 | Loss: 4.9714e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.9714e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37700 | Loss: 4.8753e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.8753e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37750 | Loss: 4.7908e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.7908e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37800 | Loss: 4.7148e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.7148e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37850 | Loss: 4.6369e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.6369e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37900 | Loss: 4.5530e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.5530e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 37950 | Loss: 4.4943e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.4943e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38000 | Loss: 4.4394e-18 | Loss_d: 0.0000e+00 | Loss_e: 4.4394e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38050 | Loss: 2.4614e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.4614e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38100 | Loss: 2.1363e-18 | Loss_d: 0.0000e+00 | Loss_e: 2.1363e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38150 | Loss: 1.8981e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.8981e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38200 | Loss: 1.7602e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.7602e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38250 | Loss: 1.5955e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.5955e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38300 | Loss: 1.4906e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.4906e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38350 | Loss: 1.4196e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.4196e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38400 | Loss: 1.3373e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.3373e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38450 | Loss: 1.2829e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.2829e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38500 | Loss: 1.2388e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.2388e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38550 | Loss: 1.2025e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.2025e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38600 | Loss: 1.1701e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.1701e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38650 | Loss: 1.1285e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.1285e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38700 | Loss: 1.0973e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.0973e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38750 | Loss: 1.0680e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.0680e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38800 | Loss: 1.0384e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.0384e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38850 | Loss: 1.0087e-18 | Loss_d: 0.0000e+00 | Loss_e: 1.0087e-17 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38900 | Loss: 9.8779e-19 | Loss_d: 0.0000e+00 | Loss_e: 9.8779e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 38950 | Loss: 9.5079e-19 | Loss_d: 0.0000e+00 | Loss_e: 9.5079e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39000 | Loss: 9.2234e-19 | Loss_d: 0.0000e+00 | Loss_e: 9.2234e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39050 | Loss: 8.3994e-19 | Loss_d: 0.0000e+00 | Loss_e: 8.3994e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39100 | Loss: 7.7902e-19 | Loss_d: 0.0000e+00 | Loss_e: 7.7902e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39150 | Loss: 7.3342e-19 | Loss_d: 0.0000e+00 | Loss_e: 7.3342e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39200 | Loss: 6.9165e-19 | Loss_d: 0.0000e+00 | Loss_e: 6.9165e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39250 | Loss: 6.4411e-19 | Loss_d: 0.0000e+00 | Loss_e: 6.4411e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39300 | Loss: 6.0954e-19 | Loss_d: 0.0000e+00 | Loss_e: 6.0954e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39350 | Loss: 5.8252e-19 | Loss_d: 0.0000e+00 | Loss_e: 5.8252e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39400 | Loss: 5.5574e-19 | Loss_d: 0.0000e+00 | Loss_e: 5.5574e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39450 | Loss: 5.2665e-19 | Loss_d: 0.0000e+00 | Loss_e: 5.2665e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39500 | Loss: 4.9888e-19 | Loss_d: 0.0000e+00 | Loss_e: 4.9888e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39550 | Loss: 4.7176e-19 | Loss_d: 0.0000e+00 | Loss_e: 4.7176e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39600 | Loss: 4.4627e-19 | Loss_d: 0.0000e+00 | Loss_e: 4.4627e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39650 | Loss: 4.2292e-19 | Loss_d: 0.0000e+00 | Loss_e: 4.2292e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39700 | Loss: 4.0827e-19 | Loss_d: 0.0000e+00 | Loss_e: 4.0827e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39750 | Loss: 3.9342e-19 | Loss_d: 0.0000e+00 | Loss_e: 3.9342e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39800 | Loss: 3.8002e-19 | Loss_d: 0.0000e+00 | Loss_e: 3.8002e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39850 | Loss: 3.6327e-19 | Loss_d: 0.0000e+00 | Loss_e: 3.6327e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39900 | Loss: 3.5326e-19 | Loss_d: 0.0000e+00 | Loss_e: 3.5326e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 39950 | Loss: 3.3588e-19 | Loss_d: 0.0000e+00 | Loss_e: 3.3588e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n",
            "Step: 40000 | Loss: 3.2384e-19 | Loss_d: 0.0000e+00 | Loss_e: 3.2384e-18 | lamb: 1.080000 | Dp: 1.0000e-15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.08\n"
          ]
        }
      ],
      "source": [
        "\"\"\"Training using KFAC\"\"\"\n",
        "# training the network using KFAC\n",
        "trained_params, loss = kfac_optimizer(keys[2], stwv_loss, kfac_config, trained_params, dataf, F0, epoch2)\n",
        "\n",
        "# generate the loss history\n",
        "loss_all = np.array(loss)\n",
        "\n",
        "# get the inferred lamb\n",
        "lamb = lamb0 + eta1 * jnp.tanh(trained_params[-1])[0]\n",
        "print(lamb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7fxAz0cav-O"
      },
      "source": [
        "# computing the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqpRtkvCauPx"
      },
      "outputs": [],
      "source": [
        "# calculate the equation residue\n",
        "f_y = lambda x: predf(trained_params, x)\n",
        "f_yx = lambda x: vectgrad(f_y, x)[0]\n",
        "\n",
        "# calculate the solution\n",
        "x_star = jnp.linspace(-jnp.pi, jnp.pi, num=1001)[:, None]\n",
        "\n",
        "y_x, y_p = vectgrad(f_y, x_star)\n",
        "Hy_p = hptrans(x_star, f_y)\n",
        "Hy_x = hptrans(x_star, f_yx)\n",
        "\n",
        "# compute the equation residue\n",
        "f = stwv_gov_eqn(f_y, x_star, lamb)\n",
        "df = stwv_deri_eqn(f_y, x_star, lamb)\n",
        "\n",
        "ExpName = f\"StokesWave_small_amp_lamd={lamb:.4f}\"\n",
        "\n",
        "mdic = {\"lamb\": np.array(lamb), \"eta\": np.array(eta1), \"x\": np.array(x_star),\n",
        "        \"y\": np.array(y_p), \"Hy\": np.array(Hy_p), \"Hy_x\": np.array(Hy_x),\n",
        "        \"y_x\": np.array(y_x), \"f\": np.array(f), \"df\": df, \"loss\": loss_all}\n",
        "FileName = ExpName + '.mat'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUueJCg6a5Hm"
      },
      "source": [
        "# Saving the output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRF1NDcza4bb"
      },
      "outputs": [],
      "source": [
        "FilePath = str(rootdir.joinpath(FileName))\n",
        "savemat(FilePath, mdic)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}